{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8245087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import DNMC, NMC, NSurv, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e897d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 500\n",
    "VAL_SIZE = 100\n",
    "\n",
    "x_train = np.random.rand(TRAIN_SIZE, 10)\n",
    "y_idx = np.random.randint(50, size=(TRAIN_SIZE))\n",
    "y_train = np.eye(50)[y_idx]\n",
    "s_train = np.random.randint(2, size=(TRAIN_SIZE))\n",
    "\n",
    "x_val = np.random.rand(VAL_SIZE, 10)\n",
    "y_idx = np.random.randint(50, size=(VAL_SIZE))\n",
    "y_val = np.eye(50)[y_idx]\n",
    "s_val = np.random.randint(2, size=(VAL_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a108d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(*arrs, batch_size=1):\n",
    "    l = len(arrs[0])\n",
    "    for ndx in range(0, l, batch_size):\n",
    "        yield (arr[ndx:min(ndx + batch_size, l)] for arr in arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3efa0f3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_model(model, n_epochs, batch_size=50, learning_rate=1e-3):\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    #@tf.function\n",
    "    def train_step(x, y, s):\n",
    "        with tf.GradientTape() as tape:\n",
    "            train_loss, train_nll = model.loss(x, y, s)\n",
    "        grads = tape.gradient(train_loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        return train_loss, train_nll\n",
    "\n",
    "    #@tf.function\n",
    "    def test_step(x, y, s):\n",
    "        val_loss, val_nll = model.loss(x, y, s)\n",
    "        return val_loss, val_nll\n",
    "\n",
    "    for epoch_idx in range(n_epochs):\n",
    "\n",
    "        print(\"\\nStart of epoch %d\" % (epoch_idx,))\n",
    "        start_time = time.time()\n",
    "\n",
    "        batch_losses = []\n",
    "        batch_nll = []\n",
    "\n",
    "        for batch_idx, (xt, yt, st) in enumerate(get_batches(x_train, y_train, s_train, batch_size=batch_size)):\n",
    "\n",
    "            train_loss, train_nll = train_step(xt, yt, st)\n",
    "\n",
    "            batch_losses.append(train_loss)\n",
    "            batch_nll.append(train_nll)\n",
    "\n",
    "        # Display metrics at the end of each epoch.\n",
    "        print('Epoch training loss: %.4f, NLL = %.4f' % (np.mean(batch_losses), np.mean(batch_nll)))\n",
    "\n",
    "        batch_losses = []\n",
    "        batch_nll = []\n",
    "\n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        for batch_idx, (xv, yv, sv) in enumerate(get_batches(x_val, y_val, s_val, batch_size=batch_size)):\n",
    "\n",
    "            val_loss, val_nll = test_step(xv, yv, sv)\n",
    "\n",
    "            batch_losses.append(val_loss)\n",
    "            batch_nll.append(val_nll)\n",
    "\n",
    "        print('Validation training loss: %.4f, NLL = %.4f' % (np.mean(batch_losses), np.mean(batch_nll)))\n",
    "        #print('Time taken: %.2fs' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4b82a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Epoch training loss: 5.2067, NLL = 4.7728\n",
      "Validation training loss: 5.2679, NLL = 4.8583\n",
      "\n",
      "Start of epoch 1\n",
      "Epoch training loss: 5.0190, NLL = 4.6280\n",
      "Validation training loss: 5.1682, NLL = 4.7990\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch training loss: 4.9091, NLL = 4.5563\n",
      "Validation training loss: 5.0927, NLL = 4.7590\n",
      "\n",
      "Start of epoch 3\n",
      "Epoch training loss: 4.8159, NLL = 4.4963\n",
      "Validation training loss: 5.0488, NLL = 4.7454\n",
      "\n",
      "Start of epoch 4\n",
      "Epoch training loss: 4.7415, NLL = 4.4501\n",
      "Validation training loss: 5.0262, NLL = 4.7483\n",
      "\n",
      "Start of epoch 5\n",
      "Epoch training loss: 4.6804, NLL = 4.4123\n",
      "Validation training loss: 5.0156, NLL = 4.7587\n",
      "\n",
      "Start of epoch 6\n",
      "Epoch training loss: 4.6274, NLL = 4.3785\n",
      "Validation training loss: 5.0038, NLL = 4.7640\n",
      "\n",
      "Start of epoch 7\n",
      "Epoch training loss: 4.5801, NLL = 4.3468\n",
      "Validation training loss: 5.0008, NLL = 4.7748\n"
     ]
    }
   ],
   "source": [
    "model = DNMC()\n",
    "train_model(model, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7253f27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Epoch training loss: 4.8718, NLL = 4.8193\n",
      "Validation training loss: 4.9568, NLL = 4.9051\n",
      "\n",
      "Start of epoch 1\n",
      "Epoch training loss: 4.7600, NLL = 4.7088\n",
      "Validation training loss: 4.8923, NLL = 4.8416\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch training loss: 4.6848, NLL = 4.6345\n",
      "Validation training loss: 4.8503, NLL = 4.8005\n",
      "\n",
      "Start of epoch 3\n",
      "Epoch training loss: 4.6286, NLL = 4.5791\n",
      "Validation training loss: 4.8230, NLL = 4.7740\n",
      "\n",
      "Start of epoch 4\n",
      "Epoch training loss: 4.5836, NLL = 4.5348\n",
      "Validation training loss: 4.8047, NLL = 4.7563\n",
      "\n",
      "Start of epoch 5\n",
      "Epoch training loss: 4.5463, NLL = 4.4981\n",
      "Validation training loss: 4.7934, NLL = 4.7456\n",
      "\n",
      "Start of epoch 6\n",
      "Epoch training loss: 4.5147, NLL = 4.4671\n",
      "Validation training loss: 4.7883, NLL = 4.7409\n",
      "\n",
      "Start of epoch 7\n",
      "Epoch training loss: 4.4883, NLL = 4.4412\n",
      "Validation training loss: 4.7878, NLL = 4.7409\n"
     ]
    }
   ],
   "source": [
    "model = NMC()\n",
    "train_model(model, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e7015e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Epoch training loss: 4.8008, NLL = 4.7652\n",
      "Validation training loss: 4.9237, NLL = 4.8886\n",
      "\n",
      "Start of epoch 1\n",
      "Epoch training loss: 4.7269, NLL = 4.6922\n",
      "Validation training loss: 4.8711, NLL = 4.8369\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch training loss: 4.6671, NLL = 4.6332\n",
      "Validation training loss: 4.8253, NLL = 4.7918\n",
      "\n",
      "Start of epoch 3\n",
      "Epoch training loss: 4.6130, NLL = 4.5797\n",
      "Validation training loss: 4.7850, NLL = 4.7520\n",
      "\n",
      "Start of epoch 4\n",
      "Epoch training loss: 4.5640, NLL = 4.5312\n",
      "Validation training loss: 4.7529, NLL = 4.7202\n",
      "\n",
      "Start of epoch 5\n",
      "Epoch training loss: 4.5216, NLL = 4.4890\n",
      "Validation training loss: 4.7311, NLL = 4.6987\n",
      "\n",
      "Start of epoch 6\n",
      "Epoch training loss: 4.4869, NLL = 4.4545\n",
      "Validation training loss: 4.7207, NLL = 4.6884\n",
      "\n",
      "Start of epoch 7\n",
      "Epoch training loss: 4.4599, NLL = 4.4278\n",
      "Validation training loss: 4.7199, NLL = 4.6878\n"
     ]
    }
   ],
   "source": [
    "model = NSurv()\n",
    "train_model(model, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f78939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Epoch training loss: 0.7148, NLL = 0.6968\n",
      "Validation training loss: 0.7120, NLL = 0.6943\n",
      "\n",
      "Start of epoch 1\n",
      "Epoch training loss: 0.7105, NLL = 0.6931\n",
      "Validation training loss: 0.7123, NLL = 0.6952\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch training loss: 0.7098, NLL = 0.6931\n",
      "Validation training loss: 0.7116, NLL = 0.6952\n",
      "\n",
      "Start of epoch 3\n",
      "Epoch training loss: 0.7086, NLL = 0.6924\n",
      "Validation training loss: 0.7106, NLL = 0.6947\n",
      "\n",
      "Start of epoch 4\n",
      "Epoch training loss: 0.7074, NLL = 0.6916\n",
      "Validation training loss: 0.7098, NLL = 0.6943\n",
      "\n",
      "Start of epoch 5\n",
      "Epoch training loss: 0.7062, NLL = 0.6910\n",
      "Validation training loss: 0.7092, NLL = 0.6942\n",
      "\n",
      "Start of epoch 6\n",
      "Epoch training loss: 0.7053, NLL = 0.6905\n",
      "Validation training loss: 0.7087, NLL = 0.6941\n",
      "\n",
      "Start of epoch 7\n",
      "Epoch training loss: 0.7045, NLL = 0.6900\n",
      "Validation training loss: 0.7083, NLL = 0.6941\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "train_model(model, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b5866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62943c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL MODEL CODE HERE; IT HAS NOW BEEN MOVED TO src/models.py\n",
    "\n",
    "class DNMC(Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 phi_layer_sizes=[100,], psi_layer_sizes=[100,], omega_layer_sizes=[100,],\n",
    "                 e_layer_sizes=[100,], t_layer_sizes=[100,], c_layer_sizes=[100,],\n",
    "                 importance_weights=[1., 1.],\n",
    "                 include_censoring_density=True,\n",
    "                 n_bins=50,\n",
    "                 activation='relu',\n",
    "                 ld=1e-3, lr=1e-3):\n",
    "        \n",
    "        super(DNMC, self).__init__()\n",
    "        \n",
    "        self.ld = ld\n",
    "        self.lr = lr\n",
    "        self.w0 = tf.convert_to_tensor(importance_weights[0], dtype=tf.float32)\n",
    "        self.w1 = tf.convert_to_tensor(importance_weights[1], dtype=tf.float32)\n",
    "        self.include_censoring_density = include_censoring_density\n",
    "        self.n_bins = n_bins\n",
    "        self.activation=activation\n",
    "        \n",
    "        self.phi_layers = [self.dense(ls) for ls in phi_layer_sizes]\n",
    "        self.psi_layers = [self.dense(ls) for ls in psi_layer_sizes]\n",
    "        self.omega_layers = [self.dense(ls) for ls in omega_layer_sizes]\n",
    "        \n",
    "        self.e_layers = [self.dense(ls) for ls in e_layer_sizes] + [Dense(1, activation='sigmoid')]\n",
    "        self.t_layers = [self.dense(ls) for ls in t_layer_sizes] + [Dense(n_bins, activation='softmax')]\n",
    "        self.c_layers = [self.dense(ls) for ls in c_layer_sizes] + [Dense(n_bins, activation='softmax')]\n",
    "        \n",
    "        self.phi_model = Sequential(self.phi_layers)\n",
    "        self.psi_model = Sequential(self.psi_layers)\n",
    "        self.omega_model = Sequential(self.omega_layers)\n",
    "        \n",
    "        self.e_model = Sequential(self.e_layers)\n",
    "        self.t_model = Sequential(self.t_layers)\n",
    "        self.c_model = Sequential(self.c_layers)\n",
    "        \n",
    "        \n",
    "    def dense(self, layer_size):\n",
    "        \n",
    "        layer = Dense(\n",
    "            layer_size,\n",
    "            activation=self.activation,\n",
    "            kernel_regularizer=regularizers.l2(self.lr),\n",
    "            bias_regularizer=regularizers.l2(self.lr))\n",
    "        \n",
    "        return(layer)\n",
    "        \n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        \n",
    "        self.phi = self.phi_model(x)\n",
    "        self.psi = self.psi_model(x)\n",
    "        self.omega = self.omega_model(x)\n",
    "        \n",
    "        self.e_pred = tf.squeeze(self.e_model(tf.concat([self.phi, self.psi], axis=-1)), axis=1)\n",
    "        self.t_pred = self.t_model(tf.concat([self.psi, self.omega], axis=-1))\n",
    "        self.c_pred = self.c_model(tf.concat([self.psi, self.omega], axis=-1))\n",
    "        \n",
    "        return self.e_pred, self.t_pred, self.c_pred\n",
    "    \n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.forward_pass(x)\n",
    "    \n",
    "    \n",
    "    def iweights(self, s):\n",
    "        return tf.cast(s, dtype=tf.float32) * self.w1 + tf.cast((1 - s), dtype=tf.float32) * self.w0\n",
    "\n",
    "    \n",
    "    def loss(self, x, y, s):\n",
    "        \n",
    "        #assert np.ndim(s) == 1\n",
    "        #assert np.ndim(y) == 2\n",
    "        #assert np.shape(y)[1] == self.n_bins\n",
    "        \n",
    "        nll = tf.reduce_mean(self.iweights(s) * self.nll(x, y, s))\n",
    "        l = nll + self.ld * tf.cast(self.mmd(x, s), dtype=tf.float32)\n",
    "        l += tf.reduce_sum(self.losses)\n",
    "        \n",
    "        return l, nll\n",
    "    \n",
    "    \n",
    "    def nll(self, x, y, s):\n",
    "        \n",
    "        #assert np.ndim(s) == 1\n",
    "        #assert np.ndim(y) == 2\n",
    "        #assert np.shape(y)[1] == self.n_bins\n",
    "        \n",
    "        yt = tf.cast(y, dtype=tf.float32)\n",
    "        \n",
    "        e_pred, t_pred, c_pred = self.forward_pass(x)\n",
    "        \n",
    "        ft = tf.reduce_sum(yt * t_pred, axis=1)\n",
    "        Ft = tf.reduce_sum(yt * tf.math.cumsum(t_pred, axis=1), axis=1)\n",
    "        fc = tf.reduce_sum(yt * c_pred, axis=1)\n",
    "        Fc = tf.reduce_sum(yt * tf.math.cumsum(c_pred, axis=1), axis=1)\n",
    "        \n",
    "        l1 = e_pred * ft\n",
    "        l2 = (1 - e_pred * (1 - Ft))\n",
    "        \n",
    "        if self.include_censoring_density:\n",
    "            l1 = l1 * Fc\n",
    "            l2 = l2 * fc\n",
    "            \n",
    "        ll = tf.cast(s, dtype=tf.float32) * tf.math.log(l1)\n",
    "        ll += tf.cast((1 - s), dtype=tf.float32) * tf.math.log(l2)\n",
    "        \n",
    "        return -1 * ll\n",
    "    \n",
    "    \n",
    "    def mmd(self, x, s, beta=1.):\n",
    "        \n",
    "        x0 = tf.boolean_mask(x, s == 0, axis=0)\n",
    "        x1 = tf.boolean_mask(x, s == 1, axis=0)\n",
    "\n",
    "        x0x0 = self._gaussian_kernel(x0, x0, beta)\n",
    "        x0x1 = self._gaussian_kernel(x0, x1, beta)\n",
    "        x1x1 = self._gaussian_kernel(x1, x1, beta)\n",
    "        \n",
    "        return tf.reduce_mean(x0x0) - 2. * tf.reduce_mean(x0x1) + tf.reduce_mean(x1x1)\n",
    "\n",
    "\n",
    "    def _gaussian_kernel(self, x1, x2, beta=1.):\n",
    "        return tf.exp(-1. * beta * tf.reduce_sum((x1[:, tf.newaxis, :] - x2[tf.newaxis, :, :]) ** 2, axis=-1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
