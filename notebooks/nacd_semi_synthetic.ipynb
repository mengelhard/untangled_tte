{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b1a328",
   "metadata": {},
   "source": [
    "# Load NACD dataset\n",
    "\n",
    "- categorical variables have already been cell coded\n",
    "- however, numeric variables have not been standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "color-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import DNMC, NMC, NSurv, MLP, discrete_ci\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f2f20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SURVIVAL</th>\n",
       "      <th>CENSORED</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>BOX1_SCORE</th>\n",
       "      <th>BOX2_SCORE</th>\n",
       "      <th>BOX3_SCORE</th>\n",
       "      <th>PERFORMANCE_STATUS</th>\n",
       "      <th>PERFORMANCE_STATUS_0</th>\n",
       "      <th>PERFORMANCE_STATUS_1</th>\n",
       "      <th>PERFORMANCE_STATUS_2</th>\n",
       "      <th>...</th>\n",
       "      <th>AGE</th>\n",
       "      <th>GRANULOCYTES</th>\n",
       "      <th>LDH_SERUM</th>\n",
       "      <th>LYMPHOCYTES</th>\n",
       "      <th>PLATELET</th>\n",
       "      <th>WBC_COUNT</th>\n",
       "      <th>CALCIUM_SERUM</th>\n",
       "      <th>HGB</th>\n",
       "      <th>CREATININE_SERUM</th>\n",
       "      <th>ALBUMIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.988528</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.463214</td>\n",
       "      <td>3.357897</td>\n",
       "      <td>0.794414</td>\n",
       "      <td>0.830929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.510860</td>\n",
       "      <td>-0.270847</td>\n",
       "      <td>0.628240</td>\n",
       "      <td>-0.862502</td>\n",
       "      <td>-0.147154</td>\n",
       "      <td>-0.504179</td>\n",
       "      <td>-1.462069</td>\n",
       "      <td>-3.192489</td>\n",
       "      <td>-0.129099</td>\n",
       "      <td>-2.537179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.059539</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.853622</td>\n",
       "      <td>0.105174</td>\n",
       "      <td>-0.809101</td>\n",
       "      <td>-0.194182</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.202266</td>\n",
       "      <td>-0.301723</td>\n",
       "      <td>-0.249949</td>\n",
       "      <td>0.412771</td>\n",
       "      <td>-0.263260</td>\n",
       "      <td>-0.196202</td>\n",
       "      <td>-0.223847</td>\n",
       "      <td>-0.029112</td>\n",
       "      <td>0.440007</td>\n",
       "      <td>-1.132692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.384151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.304796</td>\n",
       "      <td>-0.979067</td>\n",
       "      <td>-0.809101</td>\n",
       "      <td>-1.219293</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.501878</td>\n",
       "      <td>0.161420</td>\n",
       "      <td>-0.167312</td>\n",
       "      <td>0.707064</td>\n",
       "      <td>1.517029</td>\n",
       "      <td>0.419750</td>\n",
       "      <td>0.723029</td>\n",
       "      <td>0.692360</td>\n",
       "      <td>0.172192</td>\n",
       "      <td>0.037714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.941069</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.853622</td>\n",
       "      <td>0.105174</td>\n",
       "      <td>-0.541848</td>\n",
       "      <td>-0.194182</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.646601</td>\n",
       "      <td>-0.548733</td>\n",
       "      <td>-0.262554</td>\n",
       "      <td>1.688043</td>\n",
       "      <td>0.549481</td>\n",
       "      <td>-0.093544</td>\n",
       "      <td>0.358846</td>\n",
       "      <td>-0.084610</td>\n",
       "      <td>0.239146</td>\n",
       "      <td>0.974039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.267267</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.304796</td>\n",
       "      <td>0.105174</td>\n",
       "      <td>0.794414</td>\n",
       "      <td>1.856040</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.423449</td>\n",
       "      <td>-0.363476</td>\n",
       "      <td>-0.254151</td>\n",
       "      <td>-0.077719</td>\n",
       "      <td>-1.006337</td>\n",
       "      <td>-0.375855</td>\n",
       "      <td>0.140336</td>\n",
       "      <td>0.525866</td>\n",
       "      <td>-1.099927</td>\n",
       "      <td>0.505877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>-0.586857</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.463214</td>\n",
       "      <td>2.273656</td>\n",
       "      <td>-0.274596</td>\n",
       "      <td>-0.194182</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278762</td>\n",
       "      <td>0.377554</td>\n",
       "      <td>-0.132297</td>\n",
       "      <td>-0.077719</td>\n",
       "      <td>0.371452</td>\n",
       "      <td>0.291427</td>\n",
       "      <td>1.232886</td>\n",
       "      <td>2.468290</td>\n",
       "      <td>0.239146</td>\n",
       "      <td>2.144445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>-0.879964</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.274413</td>\n",
       "      <td>0.105174</td>\n",
       "      <td>0.794414</td>\n",
       "      <td>0.830929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244740</td>\n",
       "      <td>-0.054713</td>\n",
       "      <td>-0.118291</td>\n",
       "      <td>-0.077719</td>\n",
       "      <td>0.495298</td>\n",
       "      <td>-0.093544</td>\n",
       "      <td>3.636494</td>\n",
       "      <td>-0.306601</td>\n",
       "      <td>0.339577</td>\n",
       "      <td>-1.834936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>0.588593</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.463214</td>\n",
       "      <td>1.189415</td>\n",
       "      <td>1.328919</td>\n",
       "      <td>0.830929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.638837</td>\n",
       "      <td>-0.826619</td>\n",
       "      <td>-0.695346</td>\n",
       "      <td>-0.273914</td>\n",
       "      <td>-0.247779</td>\n",
       "      <td>-0.889149</td>\n",
       "      <td>0.504519</td>\n",
       "      <td>-0.750584</td>\n",
       "      <td>-0.129099</td>\n",
       "      <td>1.910364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>0.537483</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.304796</td>\n",
       "      <td>0.105174</td>\n",
       "      <td>0.259909</td>\n",
       "      <td>-0.194182</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.790857</td>\n",
       "      <td>0.130544</td>\n",
       "      <td>-0.202328</td>\n",
       "      <td>0.020379</td>\n",
       "      <td>1.168712</td>\n",
       "      <td>0.137439</td>\n",
       "      <td>0.286010</td>\n",
       "      <td>-0.417597</td>\n",
       "      <td>0.406530</td>\n",
       "      <td>1.442202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2401</th>\n",
       "      <td>-0.906387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.853622</td>\n",
       "      <td>0.105174</td>\n",
       "      <td>-0.541848</td>\n",
       "      <td>-0.194182</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491504</td>\n",
       "      <td>-0.085590</td>\n",
       "      <td>-0.377405</td>\n",
       "      <td>-0.273914</td>\n",
       "      <td>0.077317</td>\n",
       "      <td>-0.093544</td>\n",
       "      <td>-0.223847</td>\n",
       "      <td>0.137381</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.037714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2402 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SURVIVAL  CENSORED  GENDER  BOX1_SCORE  BOX2_SCORE  BOX3_SCORE  \\\n",
       "0    -0.988528         0       1    1.463214    3.357897    0.794414   \n",
       "1     1.059539         1       0   -0.853622    0.105174   -0.809101   \n",
       "2     0.384151         0       0    0.304796   -0.979067   -0.809101   \n",
       "3    -0.941069         1       0   -0.853622    0.105174   -0.541848   \n",
       "4    -0.267267         0       0    0.304796    0.105174    0.794414   \n",
       "...        ...       ...     ...         ...         ...         ...   \n",
       "2397 -0.586857         0       1    1.463214    2.273656   -0.274596   \n",
       "2398 -0.879964         0       1   -0.274413    0.105174    0.794414   \n",
       "2399  0.588593         0       1    1.463214    1.189415    1.328919   \n",
       "2400  0.537483         0       1    0.304796    0.105174    0.259909   \n",
       "2401 -0.906387         1       0   -0.853622    0.105174   -0.541848   \n",
       "\n",
       "      PERFORMANCE_STATUS  PERFORMANCE_STATUS_0  PERFORMANCE_STATUS_1  \\\n",
       "0               0.830929                     0                     0   \n",
       "1              -0.194182                     0                     1   \n",
       "2              -1.219293                     1                     0   \n",
       "3              -0.194182                     0                     1   \n",
       "4               1.856040                     0                     0   \n",
       "...                  ...                   ...                   ...   \n",
       "2397           -0.194182                     0                     1   \n",
       "2398            0.830929                     0                     0   \n",
       "2399            0.830929                     0                     0   \n",
       "2400           -0.194182                     0                     1   \n",
       "2401           -0.194182                     0                     1   \n",
       "\n",
       "      PERFORMANCE_STATUS_2  ...       AGE  GRANULOCYTES  LDH_SERUM  \\\n",
       "0                        1  ...  1.510860     -0.270847   0.628240   \n",
       "1                        0  ...  1.202266     -0.301723  -0.249949   \n",
       "2                        0  ...  1.501878      0.161420  -0.167312   \n",
       "3                        0  ...  0.646601     -0.548733  -0.262554   \n",
       "4                        0  ... -0.423449     -0.363476  -0.254151   \n",
       "...                    ...  ...       ...           ...        ...   \n",
       "2397                     0  ...  0.278762      0.377554  -0.132297   \n",
       "2398                     1  ...  0.244740     -0.054713  -0.118291   \n",
       "2399                     1  ... -0.638837     -0.826619  -0.695346   \n",
       "2400                     0  ... -0.790857      0.130544  -0.202328   \n",
       "2401                     0  ...  0.491504     -0.085590  -0.377405   \n",
       "\n",
       "      LYMPHOCYTES  PLATELET  WBC_COUNT  CALCIUM_SERUM       HGB  \\\n",
       "0       -0.862502 -0.147154  -0.504179      -1.462069 -3.192489   \n",
       "1        0.412771 -0.263260  -0.196202      -0.223847 -0.029112   \n",
       "2        0.707064  1.517029   0.419750       0.723029  0.692360   \n",
       "3        1.688043  0.549481  -0.093544       0.358846 -0.084610   \n",
       "4       -0.077719 -1.006337  -0.375855       0.140336  0.525866   \n",
       "...           ...       ...        ...            ...       ...   \n",
       "2397    -0.077719  0.371452   0.291427       1.232886  2.468290   \n",
       "2398    -0.077719  0.495298  -0.093544       3.636494 -0.306601   \n",
       "2399    -0.273914 -0.247779  -0.889149       0.504519 -0.750584   \n",
       "2400     0.020379  1.168712   0.137439       0.286010 -0.417597   \n",
       "2401    -0.273914  0.077317  -0.093544      -0.223847  0.137381   \n",
       "\n",
       "      CREATININE_SERUM   ALBUMIN  \n",
       "0            -0.129099 -2.537179  \n",
       "1             0.440007 -1.132692  \n",
       "2             0.172192  0.037714  \n",
       "3             0.239146  0.974039  \n",
       "4            -1.099927  0.505877  \n",
       "...                ...       ...  \n",
       "2397          0.239146  2.144445  \n",
       "2398          0.339577 -1.834936  \n",
       "2399         -0.129099  1.910364  \n",
       "2400          0.406530  1.442202  \n",
       "2401          0.004808  0.037714  \n",
       "\n",
       "[2402 rows x 53 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('http://pssp.srv.ualberta.ca/system/predictors/datasets/000/000/032/original/All_Data_updated_may2011_CLEANED.csv?1350302245')\n",
    "\n",
    "numrc_cols = df.nunique() > 2\n",
    "df.loc[:, numrc_cols] = (df.loc[:, numrc_cols] - df.loc[:, numrc_cols].mean()) / df.loc[:, numrc_cols].std()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3057e619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1523\n",
       "1     879\n",
       "Name: CENSORED, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CENSORED'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7e98268",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTCOMES = ['SURVIVAL', 'CENSORED']\n",
    "X = df.drop(OUTCOMES, axis=1).sample(frac=1, random_state=2021)\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be3f84e",
   "metadata": {},
   "source": [
    "## We create the semi-synthetic dataset as follows\n",
    "\n",
    "- real X\n",
    "\n",
    "- make up new failure times\n",
    "- make up new failure probabilities\n",
    "- make up new censoring times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d842bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SET GLOBAL PARAMETERS\n",
    "\n",
    "N_FEATURES = 10\n",
    "N_BINS = 10\n",
    "N_OVERLAPPING = 5\n",
    "LEARNING_RATE = 6e-4\n",
    "\n",
    "rs = np.random.RandomState(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88bb0fd",
   "metadata": {},
   "source": [
    "## Separate Factors\n",
    "\n",
    "- randomly select separate factors for failure times vs probabilities (no overlap)\n",
    "- censoring times are random uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd57065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(*arrs, nbins=10, pad=1e-5):\n",
    "    min_val = np.amin(np.stack(arrs))\n",
    "    max_val = np.amax(np.stack(arrs))\n",
    "    binsize = (max_val - min_val + pad) / nbins\n",
    "    return ((arr - arr.min()) // binsize for arr in arrs)\n",
    "    \n",
    "def onehot(arr, ncategories=None):\n",
    "    if ncategories is None:\n",
    "        ncategories = len(np.unique(arr))\n",
    "    return np.eye(ncategories)[arr.astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5897e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_order = rs.permutation(len(X.T))\n",
    "T_features = feature_order[:N_FEATURES]\n",
    "E_features = feature_order[N_FEATURES:(2 * N_FEATURES)]\n",
    "\n",
    "T_weights = rs.randn(len(T_features))\n",
    "E_weights = rs.randn(len(E_features))\n",
    "\n",
    "T = X[:, T_features] @ T_weights\n",
    "E = X[:, E_features] @ E_weights\n",
    "\n",
    "t_cont = np.exp((T - np.mean(T)) / (2 * np.std(T)))\n",
    "c_cont = rs.rand(len(t_cont)) * t_cont.max()\n",
    "\n",
    "e_prob = 1 - 1 / (1 + np.exp(4. * (E - E.mean()) / E.std()))\n",
    "e = (rs.rand(len(e_prob)) < e_prob).astype(int)\n",
    "\n",
    "ct_cont = np.stack([c_cont, t_cont]).T\n",
    "y_cont = e * np.amin(ct_cont, axis=1) + (1 - e) * c_cont\n",
    "\n",
    "s = e * np.argmin(ct_cont, axis=1)\n",
    "y_disc, t_disc, c_disc = discretize(y_cont, t_cont, c_cont, nbins=N_BINS)\n",
    "\n",
    "y = onehot(y_disc, ncategories=N_BINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da6f44a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many are susceptible?\n",
      "0    1335\n",
      "1    1067\n",
      "dtype: int64\n",
      "How many are observed?\n",
      "0    1581\n",
      "1     821\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('How many are susceptible?')\n",
    "print(pd.value_counts(e).sort_index())\n",
    "\n",
    "print('How many are observed?')\n",
    "print(pd.value_counts(s).sort_index())\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))\n",
    "\n",
    "ax[0].hist(e_prob)\n",
    "ax[1].hist(t_disc)\n",
    "ax[2].hist(c_disc)\n",
    "ax[3].hist(y_disc)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96467beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, s_train = X[:1500], y[:1500], s[:1500]\n",
    "x_val, y_val, s_val = X[1500:1900], y[1500:1900], s[1500:1900]\n",
    "x_test, y_test, s_test, e_test = X[1900:], y[1900:], s[1900:], e[1900:]\n",
    "t_disc_test, c_disc_test, y_disc_test = t_disc[1900:], c_disc[1900:], y_disc[1900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16888d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(*arrs, batch_size=1):\n",
    "    l = len(arrs[0])\n",
    "    for ndx in range(0, l, batch_size):\n",
    "        yield (arr[ndx:min(ndx + batch_size, l)] for arr in arrs)\n",
    "        \n",
    "import time\n",
    "\n",
    "def train_model(\n",
    "    model, train_data, val_data, n_epochs,\n",
    "    batch_size=50, learning_rate=1e-3, early_stopping_criterion=2):\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    #@tf.function\n",
    "    def train_step(x, y, s):\n",
    "        with tf.GradientTape() as tape:\n",
    "            train_loss, train_nll = model.loss(x, y, s)\n",
    "            #print(train_loss, train_nll)\n",
    "        grads = tape.gradient(train_loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        return train_loss, train_nll\n",
    "\n",
    "    #@tf.function\n",
    "    def test_step(x, y, s):\n",
    "        val_loss, val_nll = model.loss(x, y, s)\n",
    "        return val_loss, val_nll\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    no_decrease = 0\n",
    "\n",
    "    for epoch_idx in range(n_epochs):\n",
    "\n",
    "        #print(\"\\nStart of epoch %d\" % (epoch_idx,))\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_losses = []\n",
    "        train_nlls = []\n",
    "\n",
    "        for batch_idx, (xt, yt, st) in enumerate(get_batches(*train_data, batch_size=batch_size)):\n",
    "\n",
    "            train_loss, train_nll = train_step(xt, yt, st)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            train_nlls.append(train_nll)\n",
    "\n",
    "        # Display metrics at the end of each epoch.\n",
    "        #print('Epoch training loss: %.4f, NLL = %.4f' % (np.mean(batch_losses), np.mean(batch_nll)))\n",
    "\n",
    "        val_losses = []\n",
    "        val_nlls = []\n",
    "\n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        for batch_idx, (xv, yv, sv) in enumerate(get_batches(*val_data, batch_size=batch_size)):\n",
    "\n",
    "            val_loss, val_nll = test_step(xv, yv, sv)\n",
    "\n",
    "            val_losses.append(val_loss)\n",
    "            val_nlls.append(val_nll)\n",
    "            \n",
    "        new_val_loss = np.mean(val_losses)\n",
    "\n",
    "        print(\n",
    "            'Epoch %2i | Train Loss: %.4f | Train NLL: %.4f | Val Loss: %.4f | Val NLL: %.4f'\n",
    "            % (epoch_idx, np.mean(train_losses), np.mean(train_nlls), np.mean(val_losses), np.mean(val_nlls))\n",
    "        )\n",
    "        #print('Time taken: %.2fs' % (time.time() - start_time))\n",
    "                \n",
    "        if new_val_loss > best_val_loss:\n",
    "            no_decrease += 1\n",
    "        else:\n",
    "            no_decrease = 0\n",
    "            best_val_loss = new_val_loss\n",
    "            \n",
    "        if no_decrease == early_stopping_criterion:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbbcc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def nll(t_true, t_pred, tol=1e-8):\n",
    "    ncat = np.shape(t_pred)[1]\n",
    "    nll_ = -1 * np.log(np.sum(onehot(t_true, ncategories=ncat) * t_pred, axis=1) + tol)\n",
    "    return np.mean(nll_)\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model, test_data, e_test, \n",
    "    batch_size=50, dataset='nacd', factors='unknown'):\n",
    "    \n",
    "    modelname = type(model).__name__\n",
    "    \n",
    "    test_losses = []\n",
    "    test_nlls = []\n",
    "    \n",
    "    test_e_pred = []\n",
    "    test_t_pred = []\n",
    "    test_c_pred = []\n",
    "    \n",
    "    for batch_idx, (xt, yt, st) in enumerate(get_batches(*test_data, batch_size=batch_size)):\n",
    "        \n",
    "        test_loss, test_nll = model.loss(xt, yt, st)\n",
    "        \n",
    "        if modelname == 'NSurv':\n",
    "            t_pred, c_pred = model(xt)\n",
    "            test_t_pred.append(t_pred)\n",
    "            test_c_pred.append(c_pred)\n",
    "        elif modelname == 'MLP':\n",
    "            e_pred = model(xt)\n",
    "            test_e_pred.append(e_pred)\n",
    "        else:\n",
    "            e_pred, t_pred, c_pred = model(xt)\n",
    "            test_e_pred.append(e_pred)\n",
    "            test_t_pred.append(t_pred)\n",
    "            test_c_pred.append(c_pred)\n",
    "        \n",
    "        test_losses.append(test_loss)\n",
    "        test_nlls.append(test_nll)\n",
    "    \n",
    "    if modelname == 'NSurv':\n",
    "        e_auc = None\n",
    "    else:\n",
    "        e_auc = roc_auc_score(e_test, np.concatenate(test_e_pred, axis=0))\n",
    "        \n",
    "    if modelname == 'MLP':\n",
    "        ci = None\n",
    "    else:\n",
    "        ci = discrete_ci(test_data[2], test_data[1], np.concatenate(test_t_pred, axis=0))\n",
    "    \n",
    "    results = {\n",
    "        'dataset': dataset,\n",
    "        'factors': factors,\n",
    "        'model': modelname,\n",
    "        'ld': model.ld,\n",
    "        'lr': model.lr,\n",
    "        'avg_test_loss': np.mean(test_losses),\n",
    "        'avg_test_nll': np.mean(test_nlls),\n",
    "        'e_auc': e_auc,\n",
    "        'y_ci': ci\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0edf34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN COLLECTING RESULTS HERE ###\n",
    "all_results = []\n",
    "\n",
    "pstrue = s_train.mean()\n",
    "importance_weights = [1 / (1 - pstrue), 1 / pstrue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56257c70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 16.0778 | Train NLL: 4.8416 | Val Loss: 14.2296 | Val NLL: 4.5909\n",
      "Epoch  1 | Train Loss: 12.5194 | Train NLL: 4.1218 | Val Loss: 11.5345 | Val NLL: 4.3241\n",
      "Epoch  2 | Train Loss: 10.1551 | Train NLL: 3.8174 | Val Loss: 9.6705 | Val NLL: 4.1569\n",
      "Epoch  3 | Train Loss: 8.5076 | Train NLL: 3.5980 | Val Loss: 8.3819 | Val NLL: 4.0463\n",
      "Epoch  4 | Train Loss: 7.3420 | Train NLL: 3.4318 | Val Loss: 7.4577 | Val NLL: 3.9551\n",
      "Epoch  5 | Train Loss: 6.5012 | Train NLL: 3.3047 | Val Loss: 6.8059 | Val NLL: 3.9051\n",
      "Epoch  6 | Train Loss: 5.8798 | Train NLL: 3.2042 | Val Loss: 6.3161 | Val NLL: 3.8604\n",
      "Epoch  7 | Train Loss: 5.4099 | Train NLL: 3.1229 | Val Loss: 5.9509 | Val NLL: 3.8298\n",
      "Epoch  8 | Train Loss: 5.0454 | Train NLL: 3.0528 | Val Loss: 5.6754 | Val NLL: 3.8105\n",
      "Epoch  9 | Train Loss: 4.7582 | Train NLL: 2.9927 | Val Loss: 5.4617 | Val NLL: 3.7958\n",
      "Epoch 10 | Train Loss: 4.5290 | Train NLL: 2.9408 | Val Loss: 5.2917 | Val NLL: 3.7820\n",
      "Epoch 11 | Train Loss: 4.3420 | Train NLL: 2.8937 | Val Loss: 5.1643 | Val NLL: 3.7785\n",
      "Epoch 12 | Train Loss: 4.1872 | Train NLL: 2.8501 | Val Loss: 5.0652 | Val NLL: 3.7785\n",
      "Epoch 13 | Train Loss: 4.0576 | Train NLL: 2.8099 | Val Loss: 4.9859 | Val NLL: 3.7787\n",
      "Epoch 14 | Train Loss: 3.9481 | Train NLL: 2.7723 | Val Loss: 4.9247 | Val NLL: 3.7821\n",
      "Epoch 15 | Train Loss: 3.8547 | Train NLL: 2.7372 | Val Loss: 4.8804 | Val NLL: 3.7900\n",
      "Epoch 16 | Train Loss: 3.7747 | Train NLL: 2.7046 | Val Loss: 4.8423 | Val NLL: 3.7945\n",
      "Epoch 17 | Train Loss: 3.7024 | Train NLL: 2.6710 | Val Loss: 4.8120 | Val NLL: 3.7989\n",
      "Epoch 18 | Train Loss: 3.6399 | Train NLL: 2.6400 | Val Loss: 4.7906 | Val NLL: 3.8057\n",
      "Epoch 19 | Train Loss: 3.5824 | Train NLL: 2.6081 | Val Loss: 4.7751 | Val NLL: 3.8131\n",
      "Epoch 20 | Train Loss: 3.5304 | Train NLL: 2.5767 | Val Loss: 4.7695 | Val NLL: 3.8261\n",
      "Epoch 21 | Train Loss: 3.4839 | Train NLL: 2.5469 | Val Loss: 4.7580 | Val NLL: 3.8294\n",
      "Epoch 22 | Train Loss: 3.4398 | Train NLL: 2.5162 | Val Loss: 4.7580 | Val NLL: 3.8412\n",
      "Epoch 23 | Train Loss: 3.3997 | Train NLL: 2.4867 | Val Loss: 4.7608 | Val NLL: 3.8534\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=1e-2, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='separate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca0c0901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 15.9496 | Train NLL: 4.8215 | Val Loss: 14.0304 | Val NLL: 4.5562\n",
      "Epoch  1 | Train Loss: 12.3220 | Train NLL: 4.1001 | Val Loss: 11.3243 | Val NLL: 4.2890\n",
      "Epoch  2 | Train Loss: 9.9658 | Train NLL: 3.7924 | Val Loss: 9.5050 | Val NLL: 4.1434\n",
      "Epoch  3 | Train Loss: 8.3435 | Train NLL: 3.5737 | Val Loss: 8.2425 | Val NLL: 4.0339\n",
      "Epoch  4 | Train Loss: 7.2056 | Train NLL: 3.4117 | Val Loss: 7.3634 | Val NLL: 3.9662\n",
      "Epoch  5 | Train Loss: 6.3875 | Train NLL: 3.2873 | Val Loss: 6.7238 | Val NLL: 3.9105\n",
      "Epoch  6 | Train Loss: 5.7852 | Train NLL: 3.1897 | Val Loss: 6.2597 | Val NLL: 3.8768\n",
      "Epoch  7 | Train Loss: 5.3314 | Train NLL: 3.1115 | Val Loss: 5.9078 | Val NLL: 3.8486\n",
      "Epoch  8 | Train Loss: 4.9773 | Train NLL: 3.0420 | Val Loss: 5.6442 | Val NLL: 3.8323\n",
      "Epoch  9 | Train Loss: 4.7013 | Train NLL: 2.9849 | Val Loss: 5.4420 | Val NLL: 3.8215\n",
      "Epoch 10 | Train Loss: 4.4799 | Train NLL: 2.9339 | Val Loss: 5.2879 | Val NLL: 3.8176\n",
      "Epoch 11 | Train Loss: 4.3001 | Train NLL: 2.8884 | Val Loss: 5.1682 | Val NLL: 3.8166\n",
      "Epoch 12 | Train Loss: 4.1511 | Train NLL: 2.8457 | Val Loss: 5.0702 | Val NLL: 3.8129\n",
      "Epoch 13 | Train Loss: 4.0274 | Train NLL: 2.8070 | Val Loss: 4.9993 | Val NLL: 3.8178\n",
      "Epoch 14 | Train Loss: 3.9227 | Train NLL: 2.7705 | Val Loss: 4.9451 | Val NLL: 3.8246\n",
      "Epoch 15 | Train Loss: 3.8306 | Train NLL: 2.7335 | Val Loss: 4.9003 | Val NLL: 3.8291\n",
      "Epoch 16 | Train Loss: 3.7528 | Train NLL: 2.7004 | Val Loss: 4.8674 | Val NLL: 3.8362\n",
      "Epoch 17 | Train Loss: 3.6833 | Train NLL: 2.6671 | Val Loss: 4.8438 | Val NLL: 3.8449\n",
      "Epoch 18 | Train Loss: 3.6221 | Train NLL: 2.6352 | Val Loss: 4.8259 | Val NLL: 3.8531\n",
      "Epoch 19 | Train Loss: 3.5666 | Train NLL: 2.6033 | Val Loss: 4.8169 | Val NLL: 3.8652\n",
      "Epoch 20 | Train Loss: 3.5172 | Train NLL: 2.5728 | Val Loss: 4.8147 | Val NLL: 3.8800\n",
      "Epoch 21 | Train Loss: 3.4722 | Train NLL: 2.5431 | Val Loss: 4.8109 | Val NLL: 3.8897\n",
      "Epoch 22 | Train Loss: 3.4304 | Train NLL: 2.5134 | Val Loss: 4.8146 | Val NLL: 3.9041\n",
      "Epoch 23 | Train Loss: 3.3911 | Train NLL: 2.4837 | Val Loss: 4.8198 | Val NLL: 3.9176\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=3e-3, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='separate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e017ad45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 15.9127 | Train NLL: 4.8193 | Val Loss: 13.9673 | Val NLL: 4.5261\n",
      "Epoch  1 | Train Loss: 12.2887 | Train NLL: 4.0893 | Val Loss: 11.2979 | Val NLL: 4.2755\n",
      "Epoch  2 | Train Loss: 9.9539 | Train NLL: 3.7874 | Val Loss: 9.4893 | Val NLL: 4.1293\n",
      "Epoch  3 | Train Loss: 8.3440 | Train NLL: 3.5725 | Val Loss: 8.2427 | Val NLL: 4.0300\n",
      "Epoch  4 | Train Loss: 7.2123 | Train NLL: 3.4125 | Val Loss: 7.3677 | Val NLL: 3.9637\n",
      "Epoch  5 | Train Loss: 6.4009 | Train NLL: 3.2932 | Val Loss: 6.7337 | Val NLL: 3.9123\n",
      "Epoch  6 | Train Loss: 5.8029 | Train NLL: 3.1984 | Val Loss: 6.2676 | Val NLL: 3.8748\n",
      "Epoch  7 | Train Loss: 5.3476 | Train NLL: 3.1169 | Val Loss: 5.9237 | Val NLL: 3.8530\n",
      "Epoch  8 | Train Loss: 4.9968 | Train NLL: 3.0496 | Val Loss: 5.6670 | Val NLL: 3.8426\n",
      "Epoch  9 | Train Loss: 4.7189 | Train NLL: 2.9896 | Val Loss: 5.4689 | Val NLL: 3.8352\n",
      "Epoch 10 | Train Loss: 4.4968 | Train NLL: 2.9374 | Val Loss: 5.3128 | Val NLL: 3.8290\n",
      "Epoch 11 | Train Loss: 4.3156 | Train NLL: 2.8906 | Val Loss: 5.1923 | Val NLL: 3.8276\n",
      "Epoch 12 | Train Loss: 4.1652 | Train NLL: 2.8471 | Val Loss: 5.1012 | Val NLL: 3.8317\n",
      "Epoch 13 | Train Loss: 4.0401 | Train NLL: 2.8080 | Val Loss: 5.0241 | Val NLL: 3.8313\n",
      "Epoch 14 | Train Loss: 3.9327 | Train NLL: 2.7699 | Val Loss: 4.9726 | Val NLL: 3.8418\n",
      "Epoch 15 | Train Loss: 3.8416 | Train NLL: 2.7350 | Val Loss: 4.9264 | Val NLL: 3.8461\n",
      "Epoch 16 | Train Loss: 3.7607 | Train NLL: 2.6999 | Val Loss: 4.8910 | Val NLL: 3.8518\n",
      "Epoch 17 | Train Loss: 3.6902 | Train NLL: 2.6666 | Val Loss: 4.8670 | Val NLL: 3.8612\n",
      "Epoch 18 | Train Loss: 3.6293 | Train NLL: 2.6358 | Val Loss: 4.8529 | Val NLL: 3.8740\n",
      "Epoch 19 | Train Loss: 3.5736 | Train NLL: 2.6048 | Val Loss: 4.8394 | Val NLL: 3.8829\n",
      "Epoch 20 | Train Loss: 3.5226 | Train NLL: 2.5741 | Val Loss: 4.8343 | Val NLL: 3.8960\n",
      "Epoch 21 | Train Loss: 3.4754 | Train NLL: 2.5431 | Val Loss: 4.8292 | Val NLL: 3.9053\n",
      "Epoch 22 | Train Loss: 3.4325 | Train NLL: 2.5133 | Val Loss: 4.8349 | Val NLL: 3.9226\n",
      "Epoch 23 | Train Loss: 3.3930 | Train NLL: 2.4841 | Val Loss: 4.8368 | Val NLL: 3.9336\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=3e-2, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='separate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b4846ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 13.9900 | Train NLL: 4.9401 | Val Loss: 12.5577 | Val NLL: 4.7019\n",
      "Epoch  1 | Train Loss: 11.2405 | Train NLL: 4.3232 | Val Loss: 10.4234 | Val NLL: 4.4127\n",
      "Epoch  2 | Train Loss: 9.3308 | Train NLL: 4.0068 | Val Loss: 8.8857 | Val NLL: 4.2200\n",
      "Epoch  3 | Train Loss: 7.9433 | Train NLL: 3.7729 | Val Loss: 7.7779 | Val NLL: 4.0825\n",
      "Epoch  4 | Train Loss: 6.9303 | Train NLL: 3.5936 | Val Loss: 6.9722 | Val NLL: 3.9811\n",
      "Epoch  5 | Train Loss: 6.1826 | Train NLL: 3.4545 | Val Loss: 6.3796 | Val NLL: 3.9063\n",
      "Epoch  6 | Train Loss: 5.6226 | Train NLL: 3.3443 | Val Loss: 5.9391 | Val NLL: 3.8506\n",
      "Epoch  7 | Train Loss: 5.1962 | Train NLL: 3.2537 | Val Loss: 5.6070 | Val NLL: 3.8077\n",
      "Epoch  8 | Train Loss: 4.8654 | Train NLL: 3.1767 | Val Loss: 5.3538 | Val NLL: 3.7746\n",
      "Epoch  9 | Train Loss: 4.6046 | Train NLL: 3.1103 | Val Loss: 5.1621 | Val NLL: 3.7526\n",
      "Epoch 10 | Train Loss: 4.3958 | Train NLL: 3.0523 | Val Loss: 5.0104 | Val NLL: 3.7334\n",
      "Epoch 11 | Train Loss: 4.2256 | Train NLL: 3.0001 | Val Loss: 4.8979 | Val NLL: 3.7250\n",
      "Epoch 12 | Train Loss: 4.0844 | Train NLL: 2.9521 | Val Loss: 4.8055 | Val NLL: 3.7152\n",
      "Epoch 13 | Train Loss: 3.9662 | Train NLL: 2.9079 | Val Loss: 4.7417 | Val NLL: 3.7169\n",
      "Epoch 14 | Train Loss: 3.8653 | Train NLL: 2.8660 | Val Loss: 4.6867 | Val NLL: 3.7144\n",
      "Epoch 15 | Train Loss: 3.7783 | Train NLL: 2.8261 | Val Loss: 4.6523 | Val NLL: 3.7219\n",
      "Epoch 16 | Train Loss: 3.7021 | Train NLL: 2.7876 | Val Loss: 4.6156 | Val NLL: 3.7187\n",
      "Epoch 17 | Train Loss: 3.6344 | Train NLL: 2.7500 | Val Loss: 4.6021 | Val NLL: 3.7320\n",
      "Epoch 18 | Train Loss: 3.5742 | Train NLL: 2.7136 | Val Loss: 4.5796 | Val NLL: 3.7305\n",
      "Epoch 19 | Train Loss: 3.5191 | Train NLL: 2.6772 | Val Loss: 4.5790 | Val NLL: 3.7463\n",
      "Epoch 20 | Train Loss: 3.4687 | Train NLL: 2.6414 | Val Loss: 4.5672 | Val NLL: 3.7473\n",
      "Epoch 21 | Train Loss: 3.4215 | Train NLL: 2.6055 | Val Loss: 4.5764 | Val NLL: 3.7660\n",
      "Epoch 22 | Train Loss: 3.3783 | Train NLL: 2.5705 | Val Loss: 4.5700 | Val NLL: 3.7666\n"
     ]
    }
   ],
   "source": [
    "model = NMC(n_bins=N_BINS, importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='separate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3bdba0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 11.0753 | Train NLL: 4.9429 | Val Loss: 10.1742 | Val NLL: 4.7113\n",
      "Epoch  1 | Train Loss: 9.2315 | Train NLL: 4.3101 | Val Loss: 8.7934 | Val NLL: 4.4036\n",
      "Epoch  2 | Train Loss: 7.9809 | Train NLL: 4.0067 | Val Loss: 7.7984 | Val NLL: 4.2289\n",
      "Epoch  3 | Train Loss: 7.0637 | Train NLL: 3.8076 | Val Loss: 7.0636 | Val NLL: 4.1123\n",
      "Epoch  4 | Train Loss: 6.3686 | Train NLL: 3.6532 | Val Loss: 6.5112 | Val NLL: 4.0265\n",
      "Epoch  5 | Train Loss: 5.8340 | Train NLL: 3.5289 | Val Loss: 6.0832 | Val NLL: 3.9542\n",
      "Epoch  6 | Train Loss: 5.4163 | Train NLL: 3.4253 | Val Loss: 5.7487 | Val NLL: 3.8936\n",
      "Epoch  7 | Train Loss: 5.0835 | Train NLL: 3.3354 | Val Loss: 5.4922 | Val NLL: 3.8503\n",
      "Epoch  8 | Train Loss: 4.8187 | Train NLL: 3.2608 | Val Loss: 5.2913 | Val NLL: 3.8171\n",
      "Epoch  9 | Train Loss: 4.6017 | Train NLL: 3.1940 | Val Loss: 5.1350 | Val NLL: 3.7937\n",
      "Epoch 10 | Train Loss: 4.4233 | Train NLL: 3.1350 | Val Loss: 5.0114 | Val NLL: 3.7764\n",
      "Epoch 11 | Train Loss: 4.2722 | Train NLL: 3.0795 | Val Loss: 4.9188 | Val NLL: 3.7690\n",
      "Epoch 12 | Train Loss: 4.1448 | Train NLL: 3.0292 | Val Loss: 4.8412 | Val NLL: 3.7602\n",
      "Epoch 13 | Train Loss: 4.0345 | Train NLL: 2.9810 | Val Loss: 4.7870 | Val NLL: 3.7615\n",
      "Epoch 14 | Train Loss: 3.9394 | Train NLL: 2.9362 | Val Loss: 4.7346 | Val NLL: 3.7541\n",
      "Epoch 15 | Train Loss: 3.8544 | Train NLL: 2.8918 | Val Loss: 4.7066 | Val NLL: 3.7624\n",
      "Epoch 16 | Train Loss: 3.7811 | Train NLL: 2.8514 | Val Loss: 4.6742 | Val NLL: 3.7593\n",
      "Epoch 17 | Train Loss: 3.7131 | Train NLL: 2.8097 | Val Loss: 4.6622 | Val NLL: 3.7708\n",
      "Epoch 18 | Train Loss: 3.6519 | Train NLL: 2.7697 | Val Loss: 4.6410 | Val NLL: 3.7684\n",
      "Epoch 19 | Train Loss: 3.5959 | Train NLL: 2.7305 | Val Loss: 4.6425 | Val NLL: 3.7846\n",
      "Epoch 20 | Train Loss: 3.5442 | Train NLL: 2.6919 | Val Loss: 4.6309 | Val NLL: 3.7845\n",
      "Epoch 21 | Train Loss: 3.4963 | Train NLL: 2.6541 | Val Loss: 4.6419 | Val NLL: 3.8041\n",
      "Epoch 22 | Train Loss: 3.4512 | Train NLL: 2.6163 | Val Loss: 4.6389 | Val NLL: 3.8073\n"
     ]
    }
   ],
   "source": [
    "model = NSurv(n_bins=N_BINS, importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='separate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbfcc00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 4.2602 | Train NLL: 1.2552 | Val Loss: 3.7882 | Val NLL: 1.1885\n",
      "Epoch  1 | Train Loss: 3.3650 | Train NLL: 1.0860 | Val Loss: 3.0889 | Val NLL: 1.1186\n",
      "Epoch  2 | Train Loss: 2.7444 | Train NLL: 1.0061 | Val Loss: 2.5899 | Val NLL: 1.0732\n",
      "Epoch  3 | Train Loss: 2.2956 | Train NLL: 0.9441 | Val Loss: 2.2335 | Val NLL: 1.0402\n",
      "Epoch  4 | Train Loss: 1.9689 | Train NLL: 0.8940 | Val Loss: 1.9776 | Val NLL: 1.0164\n",
      "Epoch  5 | Train Loss: 1.7295 | Train NLL: 0.8540 | Val Loss: 1.7929 | Val NLL: 1.0002\n",
      "Epoch  6 | Train Loss: 1.5519 | Train NLL: 0.8222 | Val Loss: 1.6603 | Val NLL: 0.9920\n",
      "Epoch  7 | Train Loss: 1.4178 | Train NLL: 0.7964 | Val Loss: 1.5625 | Val NLL: 0.9871\n",
      "Epoch  8 | Train Loss: 1.3145 | Train NLL: 0.7746 | Val Loss: 1.4917 | Val NLL: 0.9868\n",
      "Epoch  9 | Train Loss: 1.2329 | Train NLL: 0.7551 | Val Loss: 1.4386 | Val NLL: 0.9879\n",
      "Epoch 10 | Train Loss: 1.1672 | Train NLL: 0.7373 | Val Loss: 1.4000 | Val NLL: 0.9911\n",
      "Epoch 11 | Train Loss: 1.1135 | Train NLL: 0.7208 | Val Loss: 1.3723 | Val NLL: 0.9961\n",
      "Epoch 12 | Train Loss: 1.0684 | Train NLL: 0.7049 | Val Loss: 1.3536 | Val NLL: 1.0030\n",
      "Epoch 13 | Train Loss: 1.0303 | Train NLL: 0.6897 | Val Loss: 1.3432 | Val NLL: 1.0128\n",
      "Epoch 14 | Train Loss: 0.9971 | Train NLL: 0.6745 | Val Loss: 1.3358 | Val NLL: 1.0213\n",
      "Epoch 15 | Train Loss: 0.9680 | Train NLL: 0.6596 | Val Loss: 1.3326 | Val NLL: 1.0305\n",
      "Epoch 16 | Train Loss: 0.9415 | Train NLL: 0.6442 | Val Loss: 1.3341 | Val NLL: 1.0417\n",
      "Epoch 17 | Train Loss: 0.9179 | Train NLL: 0.6293 | Val Loss: 1.3359 | Val NLL: 1.0513\n"
     ]
    }
   ],
   "source": [
    "model = MLP(importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='separate'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4522cc",
   "metadata": {},
   "source": [
    "## Overlapping Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc74de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_order = rs.permutation(len(X.T))\n",
    "T_features = feature_order[N_OVERLAPPING:(N_FEATURES + N_OVERLAPPING)]\n",
    "E_features = feature_order[N_FEATURES:(2 * N_FEATURES)]\n",
    "\n",
    "T_weights = rs.randn(len(T_features))\n",
    "E_weights = rs.randn(len(E_features))\n",
    "\n",
    "T = X[:, T_features] @ T_weights\n",
    "E = X[:, E_features] @ E_weights\n",
    "\n",
    "t_cont = np.exp((T - np.mean(T)) / (2 * np.std(T)))\n",
    "c_cont = rs.rand(len(t_cont)) * t_cont.max()\n",
    "\n",
    "e_prob = 1 - 1 / (1 + np.exp(4. * (E - E.mean()) / E.std()))\n",
    "e = (rs.rand(len(e_prob)) < e_prob).astype(int)\n",
    "\n",
    "ct_cont = np.stack([c_cont, t_cont]).T\n",
    "y_cont = e * np.amin(ct_cont, axis=1) + (1 - e) * c_cont\n",
    "\n",
    "s = e * np.argmin(ct_cont, axis=1)\n",
    "y_disc, t_disc, c_disc = discretize(y_cont, t_cont, c_cont, nbins=N_BINS)\n",
    "\n",
    "y = onehot(y_disc, ncategories=N_BINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4398909",
   "metadata": {},
   "outputs": [],
   "source": [
    "pstrue = s_train.mean()\n",
    "importance_weights = [1 / (1 - pstrue), 1 / pstrue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac090f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many are susceptible?\n",
      "0    1130\n",
      "1    1272\n",
      "dtype: int64\n",
      "How many are observed?\n",
      "0    1468\n",
      "1     934\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('How many are susceptible?')\n",
    "print(pd.value_counts(e).sort_index())\n",
    "\n",
    "print('How many are observed?')\n",
    "print(pd.value_counts(s).sort_index())\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))\n",
    "\n",
    "ax[0].hist(e_prob)\n",
    "ax[1].hist(t_disc)\n",
    "ax[2].hist(c_disc)\n",
    "ax[3].hist(y_disc)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88e499b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, s_train = X[:1500], y[:1500], s[:1500]\n",
    "x_val, y_val, s_val = X[1500:1900], y[1500:1900], s[1500:1900]\n",
    "x_test, y_test, s_test, e_test = X[1900:], y[1900:], s[1900:], e[1900:]\n",
    "t_disc_test, c_disc_test, y_disc_test = t_disc[1900:], c_disc[1900:], y_disc[1900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59285955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 51), (1500, 10), (1500,), (400, 51), (400, 10), (400,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, s_train.shape, x_val.shape, y_val.shape, s_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2df98f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 15.7654 | Train NLL: 4.5949 | Val Loss: 13.7147 | Val NLL: 4.1408\n",
      "Epoch  1 | Train Loss: 12.1597 | Train NLL: 3.8107 | Val Loss: 11.0557 | Val NLL: 3.8742\n",
      "Epoch  2 | Train Loss: 9.8389 | Train NLL: 3.5187 | Val Loss: 9.2427 | Val NLL: 3.7362\n",
      "Epoch  3 | Train Loss: 8.2391 | Train NLL: 3.3341 | Val Loss: 7.9912 | Val NLL: 3.6573\n",
      "Epoch  4 | Train Loss: 7.1116 | Train NLL: 3.2058 | Val Loss: 7.1055 | Val NLL: 3.6081\n",
      "Epoch  5 | Train Loss: 6.2954 | Train NLL: 3.1088 | Val Loss: 6.4532 | Val NLL: 3.5646\n",
      "Epoch  6 | Train Loss: 5.6891 | Train NLL: 3.0311 | Val Loss: 5.9759 | Val NLL: 3.5396\n",
      "Epoch  7 | Train Loss: 5.2290 | Train NLL: 2.9669 | Val Loss: 5.6114 | Val NLL: 3.5173\n",
      "Epoch  8 | Train Loss: 4.8712 | Train NLL: 2.9111 | Val Loss: 5.3312 | Val NLL: 3.5004\n",
      "Epoch  9 | Train Loss: 4.5886 | Train NLL: 2.8621 | Val Loss: 5.1160 | Val NLL: 3.4901\n",
      "Epoch 10 | Train Loss: 4.3651 | Train NLL: 2.8213 | Val Loss: 4.9448 | Val NLL: 3.4801\n",
      "Epoch 11 | Train Loss: 4.1810 | Train NLL: 2.7819 | Val Loss: 4.8133 | Val NLL: 3.4769\n",
      "Epoch 12 | Train Loss: 4.0300 | Train NLL: 2.7463 | Val Loss: 4.7074 | Val NLL: 3.4739\n",
      "Epoch 13 | Train Loss: 3.9044 | Train NLL: 2.7136 | Val Loss: 4.6251 | Val NLL: 3.4743\n",
      "Epoch 14 | Train Loss: 3.7987 | Train NLL: 2.6827 | Val Loss: 4.5535 | Val NLL: 3.4699\n",
      "Epoch 15 | Train Loss: 3.7067 | Train NLL: 2.6513 | Val Loss: 4.5073 | Val NLL: 3.4775\n",
      "Epoch 16 | Train Loss: 3.6295 | Train NLL: 2.6232 | Val Loss: 4.4541 | Val NLL: 3.4689\n",
      "Epoch 17 | Train Loss: 3.5592 | Train NLL: 2.5935 | Val Loss: 4.4222 | Val NLL: 3.4732\n",
      "Epoch 18 | Train Loss: 3.4993 | Train NLL: 2.5666 | Val Loss: 4.3937 | Val NLL: 3.4746\n",
      "Epoch 19 | Train Loss: 3.4454 | Train NLL: 2.5396 | Val Loss: 4.3747 | Val NLL: 3.4795\n",
      "Epoch 20 | Train Loss: 3.3965 | Train NLL: 2.5125 | Val Loss: 4.3630 | Val NLL: 3.4875\n",
      "Epoch 21 | Train Loss: 3.3517 | Train NLL: 2.4855 | Val Loss: 4.3538 | Val NLL: 3.4941\n",
      "Epoch 22 | Train Loss: 3.3111 | Train NLL: 2.4593 | Val Loss: 4.3485 | Val NLL: 3.5016\n",
      "Epoch 23 | Train Loss: 3.2737 | Train NLL: 2.4332 | Val Loss: 4.3460 | Val NLL: 3.5091\n",
      "Epoch 24 | Train Loss: 3.2391 | Train NLL: 2.4075 | Val Loss: 4.3463 | Val NLL: 3.5171\n",
      "Epoch 25 | Train Loss: 3.2060 | Train NLL: 2.3814 | Val Loss: 4.3500 | Val NLL: 3.5269\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=1e-2, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='overlapping'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e022434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 15.8835 | Train NLL: 4.6954 | Val Loss: 13.7793 | Val NLL: 4.1892\n",
      "Epoch  1 | Train Loss: 12.2048 | Train NLL: 3.8565 | Val Loss: 11.0314 | Val NLL: 3.8671\n",
      "Epoch  2 | Train Loss: 9.8427 | Train NLL: 3.5504 | Val Loss: 9.2146 | Val NLL: 3.7451\n",
      "Epoch  3 | Train Loss: 8.2289 | Train NLL: 3.3645 | Val Loss: 7.9600 | Val NLL: 3.6686\n",
      "Epoch  4 | Train Loss: 7.0982 | Train NLL: 3.2337 | Val Loss: 7.0747 | Val NLL: 3.6167\n",
      "Epoch  5 | Train Loss: 6.2844 | Train NLL: 3.1338 | Val Loss: 6.4327 | Val NLL: 3.5769\n",
      "Epoch  6 | Train Loss: 5.6794 | Train NLL: 3.0503 | Val Loss: 5.9532 | Val NLL: 3.5425\n",
      "Epoch  7 | Train Loss: 5.2229 | Train NLL: 2.9829 | Val Loss: 5.5916 | Val NLL: 3.5168\n",
      "Epoch  8 | Train Loss: 4.8697 | Train NLL: 2.9261 | Val Loss: 5.3184 | Val NLL: 3.5017\n",
      "Epoch  9 | Train Loss: 4.5917 | Train NLL: 2.8774 | Val Loss: 5.1035 | Val NLL: 3.4884\n",
      "Epoch 10 | Train Loss: 4.3673 | Train NLL: 2.8330 | Val Loss: 4.9366 | Val NLL: 3.4803\n",
      "Epoch 11 | Train Loss: 4.1853 | Train NLL: 2.7933 | Val Loss: 4.8012 | Val NLL: 3.4711\n",
      "Epoch 12 | Train Loss: 4.0359 | Train NLL: 2.7580 | Val Loss: 4.6974 | Val NLL: 3.4692\n",
      "Epoch 13 | Train Loss: 3.9092 | Train NLL: 2.7232 | Val Loss: 4.6148 | Val NLL: 3.4686\n",
      "Epoch 14 | Train Loss: 3.8046 | Train NLL: 2.6928 | Val Loss: 4.5415 | Val NLL: 3.4618\n",
      "Epoch 15 | Train Loss: 3.7147 | Train NLL: 2.6632 | Val Loss: 4.4874 | Val NLL: 3.4617\n",
      "Epoch 16 | Train Loss: 3.6361 | Train NLL: 2.6338 | Val Loss: 4.4466 | Val NLL: 3.4652\n",
      "Epoch 17 | Train Loss: 3.5683 | Train NLL: 2.6061 | Val Loss: 4.4119 | Val NLL: 3.4661\n",
      "Epoch 18 | Train Loss: 3.5097 | Train NLL: 2.5802 | Val Loss: 4.3838 | Val NLL: 3.4678\n",
      "Epoch 19 | Train Loss: 3.4555 | Train NLL: 2.5529 | Val Loss: 4.3618 | Val NLL: 3.4698\n",
      "Epoch 20 | Train Loss: 3.4069 | Train NLL: 2.5260 | Val Loss: 4.3480 | Val NLL: 3.4754\n",
      "Epoch 21 | Train Loss: 3.3639 | Train NLL: 2.5006 | Val Loss: 4.3365 | Val NLL: 3.4796\n",
      "Epoch 22 | Train Loss: 3.3245 | Train NLL: 2.4754 | Val Loss: 4.3287 | Val NLL: 3.4846\n",
      "Epoch 23 | Train Loss: 3.2870 | Train NLL: 2.4494 | Val Loss: 4.3250 | Val NLL: 3.4910\n",
      "Epoch 24 | Train Loss: 3.2524 | Train NLL: 2.4239 | Val Loss: 4.3260 | Val NLL: 3.4997\n",
      "Epoch 25 | Train Loss: 3.2231 | Train NLL: 2.4013 | Val Loss: 4.3260 | Val NLL: 3.5055\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=3e-3, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='overlapping'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b27c03d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 15.9304 | Train NLL: 4.6981 | Val Loss: 13.8646 | Val NLL: 4.1987\n",
      "Epoch  1 | Train Loss: 12.3091 | Train NLL: 3.8711 | Val Loss: 11.1613 | Val NLL: 3.8998\n",
      "Epoch  2 | Train Loss: 9.9467 | Train NLL: 3.5579 | Val Loss: 9.3309 | Val NLL: 3.7693\n",
      "Epoch  3 | Train Loss: 8.3179 | Train NLL: 3.3693 | Val Loss: 8.0589 | Val NLL: 3.6933\n",
      "Epoch  4 | Train Loss: 7.1683 | Train NLL: 3.2388 | Val Loss: 7.1576 | Val NLL: 3.6448\n",
      "Epoch  5 | Train Loss: 6.3373 | Train NLL: 3.1401 | Val Loss: 6.4980 | Val NLL: 3.6041\n",
      "Epoch  6 | Train Loss: 5.7193 | Train NLL: 3.0583 | Val Loss: 6.0143 | Val NLL: 3.5783\n",
      "Epoch  7 | Train Loss: 5.2535 | Train NLL: 2.9929 | Val Loss: 5.6442 | Val NLL: 3.5536\n",
      "Epoch  8 | Train Loss: 4.8933 | Train NLL: 2.9367 | Val Loss: 5.3636 | Val NLL: 3.5374\n",
      "Epoch  9 | Train Loss: 4.6109 | Train NLL: 2.8888 | Val Loss: 5.1477 | Val NLL: 3.5270\n",
      "Epoch 10 | Train Loss: 4.3821 | Train NLL: 2.8432 | Val Loss: 4.9792 | Val NLL: 3.5200\n",
      "Epoch 11 | Train Loss: 4.1977 | Train NLL: 2.8032 | Val Loss: 4.8475 | Val NLL: 3.5160\n",
      "Epoch 12 | Train Loss: 4.0481 | Train NLL: 2.7686 | Val Loss: 4.7380 | Val NLL: 3.5086\n",
      "Epoch 13 | Train Loss: 3.9225 | Train NLL: 2.7352 | Val Loss: 4.6567 | Val NLL: 3.5096\n",
      "Epoch 14 | Train Loss: 3.8166 | Train NLL: 2.7034 | Val Loss: 4.5881 | Val NLL: 3.5072\n",
      "Epoch 15 | Train Loss: 3.7263 | Train NLL: 2.6733 | Val Loss: 4.5334 | Val NLL: 3.5065\n",
      "Epoch 16 | Train Loss: 3.6481 | Train NLL: 2.6441 | Val Loss: 4.4876 | Val NLL: 3.5047\n",
      "Epoch 17 | Train Loss: 3.5810 | Train NLL: 2.6170 | Val Loss: 4.4547 | Val NLL: 3.5078\n",
      "Epoch 18 | Train Loss: 3.5209 | Train NLL: 2.5896 | Val Loss: 4.4257 | Val NLL: 3.5081\n",
      "Epoch 19 | Train Loss: 3.4673 | Train NLL: 2.5626 | Val Loss: 4.4088 | Val NLL: 3.5150\n",
      "Epoch 20 | Train Loss: 3.4196 | Train NLL: 2.5365 | Val Loss: 4.3907 | Val NLL: 3.5163\n",
      "Epoch 21 | Train Loss: 3.3761 | Train NLL: 2.5106 | Val Loss: 4.3824 | Val NLL: 3.5237\n",
      "Epoch 22 | Train Loss: 3.3350 | Train NLL: 2.4836 | Val Loss: 4.3737 | Val NLL: 3.5274\n",
      "Epoch 23 | Train Loss: 3.2988 | Train NLL: 2.4586 | Val Loss: 4.3695 | Val NLL: 3.5330\n",
      "Epoch 24 | Train Loss: 3.2627 | Train NLL: 2.4312 | Val Loss: 4.3661 | Val NLL: 3.5373\n",
      "Epoch 25 | Train Loss: 3.2301 | Train NLL: 2.4054 | Val Loss: 4.3681 | Val NLL: 3.5451\n",
      "Epoch 26 | Train Loss: 3.2003 | Train NLL: 2.3806 | Val Loss: 4.3704 | Val NLL: 3.5515\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=3e-2, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='overlapping'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "608cc0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 13.9120 | Train NLL: 4.8053 | Val Loss: 12.2391 | Val NLL: 4.3127\n",
      "Epoch  1 | Train Loss: 11.0075 | Train NLL: 4.0149 | Val Loss: 10.0219 | Val NLL: 3.9297\n",
      "Epoch  2 | Train Loss: 9.0728 | Train NLL: 3.6652 | Val Loss: 8.4937 | Val NLL: 3.7438\n",
      "Epoch  3 | Train Loss: 7.7098 | Train NLL: 3.4589 | Val Loss: 7.4117 | Val NLL: 3.6402\n",
      "Epoch  4 | Train Loss: 6.7274 | Train NLL: 3.3211 | Val Loss: 6.6278 | Val NLL: 3.5734\n",
      "Epoch  5 | Train Loss: 6.0071 | Train NLL: 3.2229 | Val Loss: 6.0499 | Val NLL: 3.5266\n",
      "Epoch  6 | Train Loss: 5.4664 | Train NLL: 3.1450 | Val Loss: 5.6207 | Val NLL: 3.4948\n",
      "Epoch  7 | Train Loss: 5.0547 | Train NLL: 3.0814 | Val Loss: 5.2928 | Val NLL: 3.4676\n",
      "Epoch  8 | Train Loss: 4.7328 | Train NLL: 3.0247 | Val Loss: 5.0442 | Val NLL: 3.4497\n",
      "Epoch  9 | Train Loss: 4.4803 | Train NLL: 2.9768 | Val Loss: 4.8492 | Val NLL: 3.4339\n",
      "Epoch 10 | Train Loss: 4.2774 | Train NLL: 2.9332 | Val Loss: 4.7009 | Val NLL: 3.4259\n",
      "Epoch 11 | Train Loss: 4.1117 | Train NLL: 2.8931 | Val Loss: 4.5797 | Val NLL: 3.4157\n",
      "Epoch 12 | Train Loss: 3.9754 | Train NLL: 2.8566 | Val Loss: 4.4851 | Val NLL: 3.4096\n",
      "Epoch 13 | Train Loss: 3.8604 | Train NLL: 2.8212 | Val Loss: 4.4060 | Val NLL: 3.4014\n",
      "Epoch 14 | Train Loss: 3.7627 | Train NLL: 2.7875 | Val Loss: 4.3482 | Val NLL: 3.4006\n",
      "Epoch 15 | Train Loss: 3.6784 | Train NLL: 2.7547 | Val Loss: 4.3005 | Val NLL: 3.3989\n",
      "Epoch 16 | Train Loss: 3.6052 | Train NLL: 2.7230 | Val Loss: 4.2649 | Val NLL: 3.4004\n",
      "Epoch 17 | Train Loss: 3.5405 | Train NLL: 2.6920 | Val Loss: 4.2360 | Val NLL: 3.4015\n",
      "Epoch 18 | Train Loss: 3.4826 | Train NLL: 2.6611 | Val Loss: 4.2139 | Val NLL: 3.4035\n",
      "Epoch 19 | Train Loss: 3.4297 | Train NLL: 2.6301 | Val Loss: 4.1977 | Val NLL: 3.4066\n",
      "Epoch 20 | Train Loss: 3.3819 | Train NLL: 2.5995 | Val Loss: 4.1884 | Val NLL: 3.4126\n",
      "Epoch 21 | Train Loss: 3.3386 | Train NLL: 2.5699 | Val Loss: 4.1798 | Val NLL: 3.4158\n",
      "Epoch 22 | Train Loss: 3.2980 | Train NLL: 2.5398 | Val Loss: 4.1775 | Val NLL: 3.4226\n",
      "Epoch 23 | Train Loss: 3.2592 | Train NLL: 2.5088 | Val Loss: 4.1754 | Val NLL: 3.4271\n",
      "Epoch 24 | Train Loss: 3.2232 | Train NLL: 2.4784 | Val Loss: 4.1790 | Val NLL: 3.4353\n",
      "Epoch 25 | Train Loss: 3.1885 | Train NLL: 2.4476 | Val Loss: 4.1840 | Val NLL: 3.4432\n"
     ]
    }
   ],
   "source": [
    "model = NMC(n_bins=N_BINS, importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='overlapping'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a26ef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 10.9550 | Train NLL: 4.8098 | Val Loss: 9.8376 | Val NLL: 4.3649\n",
      "Epoch  1 | Train Loss: 9.0074 | Train NLL: 4.0782 | Val Loss: 8.4099 | Val NLL: 4.0141\n",
      "Epoch  2 | Train Loss: 7.7286 | Train NLL: 3.7457 | Val Loss: 7.4178 | Val NLL: 3.8363\n",
      "Epoch  3 | Train Loss: 6.8048 | Train NLL: 3.5346 | Val Loss: 6.6956 | Val NLL: 3.7290\n",
      "Epoch  4 | Train Loss: 6.1191 | Train NLL: 3.3895 | Val Loss: 6.1569 | Val NLL: 3.6590\n",
      "Epoch  5 | Train Loss: 5.5993 | Train NLL: 3.2835 | Val Loss: 5.7467 | Val NLL: 3.6093\n",
      "Epoch  6 | Train Loss: 5.1969 | Train NLL: 3.2004 | Val Loss: 5.4293 | Val NLL: 3.5711\n",
      "Epoch  7 | Train Loss: 4.8789 | Train NLL: 3.1309 | Val Loss: 5.1794 | Val NLL: 3.5399\n",
      "Epoch  8 | Train Loss: 4.6234 | Train NLL: 3.0711 | Val Loss: 4.9840 | Val NLL: 3.5176\n",
      "Epoch  9 | Train Loss: 4.4157 | Train NLL: 3.0187 | Val Loss: 4.8243 | Val NLL: 3.4957\n",
      "Epoch 10 | Train Loss: 4.2435 | Train NLL: 2.9708 | Val Loss: 4.7018 | Val NLL: 3.4842\n",
      "Epoch 11 | Train Loss: 4.0998 | Train NLL: 2.9272 | Val Loss: 4.5999 | Val NLL: 3.4720\n",
      "Epoch 12 | Train Loss: 3.9777 | Train NLL: 2.8865 | Val Loss: 4.5207 | Val NLL: 3.4657\n",
      "Epoch 13 | Train Loss: 3.8734 | Train NLL: 2.8485 | Val Loss: 4.4566 | Val NLL: 3.4610\n",
      "Epoch 14 | Train Loss: 3.7832 | Train NLL: 2.8122 | Val Loss: 4.4063 | Val NLL: 3.4593\n",
      "Epoch 15 | Train Loss: 3.7040 | Train NLL: 2.7770 | Val Loss: 4.3664 | Val NLL: 3.4590\n",
      "Epoch 16 | Train Loss: 3.6341 | Train NLL: 2.7433 | Val Loss: 4.3344 | Val NLL: 3.4595\n",
      "Epoch 17 | Train Loss: 3.5721 | Train NLL: 2.7109 | Val Loss: 4.3074 | Val NLL: 3.4592\n",
      "Epoch 18 | Train Loss: 3.5143 | Train NLL: 2.6773 | Val Loss: 4.2913 | Val NLL: 3.4647\n",
      "Epoch 19 | Train Loss: 3.4632 | Train NLL: 2.6457 | Val Loss: 4.2769 | Val NLL: 3.4677\n",
      "Epoch 20 | Train Loss: 3.4157 | Train NLL: 2.6139 | Val Loss: 4.2669 | Val NLL: 3.4716\n",
      "Epoch 21 | Train Loss: 3.3718 | Train NLL: 2.5824 | Val Loss: 4.2600 | Val NLL: 3.4755\n",
      "Epoch 22 | Train Loss: 3.3309 | Train NLL: 2.5510 | Val Loss: 4.2604 | Val NLL: 3.4842\n",
      "Epoch 23 | Train Loss: 3.2930 | Train NLL: 2.5204 | Val Loss: 4.2594 | Val NLL: 3.4894\n",
      "Epoch 24 | Train Loss: 3.2572 | Train NLL: 2.4899 | Val Loss: 4.2635 | Val NLL: 3.4978\n",
      "Epoch 25 | Train Loss: 3.2227 | Train NLL: 2.4590 | Val Loss: 4.2689 | Val NLL: 3.5061\n"
     ]
    }
   ],
   "source": [
    "model = NSurv(n_bins=N_BINS, importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='overlapping'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0524155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 4.2018 | Train NLL: 1.2078 | Val Loss: 3.8075 | Val NLL: 1.2212\n",
      "Epoch  1 | Train Loss: 3.2771 | Train NLL: 1.0117 | Val Loss: 3.1514 | Val NLL: 1.1960\n",
      "Epoch  2 | Train Loss: 2.6737 | Train NLL: 0.9522 | Val Loss: 2.6752 | Val NLL: 1.1768\n",
      "Epoch  3 | Train Loss: 2.2461 | Train NLL: 0.9148 | Val Loss: 2.3374 | Val NLL: 1.1658\n",
      "Epoch  4 | Train Loss: 1.9386 | Train NLL: 0.8873 | Val Loss: 2.0942 | Val NLL: 1.1580\n",
      "Epoch  5 | Train Loss: 1.7140 | Train NLL: 0.8652 | Val Loss: 1.9175 | Val NLL: 1.1523\n",
      "Epoch  6 | Train Loss: 1.5476 | Train NLL: 0.8466 | Val Loss: 1.7869 | Val NLL: 1.1474\n",
      "Epoch  7 | Train Loss: 1.4214 | Train NLL: 0.8295 | Val Loss: 1.6898 | Val NLL: 1.1438\n",
      "Epoch  8 | Train Loss: 1.3242 | Train NLL: 0.8142 | Val Loss: 1.6178 | Val NLL: 1.1424\n",
      "Epoch  9 | Train Loss: 1.2478 | Train NLL: 0.8000 | Val Loss: 1.5631 | Val NLL: 1.1418\n",
      "Epoch 10 | Train Loss: 1.1857 | Train NLL: 0.7858 | Val Loss: 1.5240 | Val NLL: 1.1445\n",
      "Epoch 11 | Train Loss: 1.1350 | Train NLL: 0.7724 | Val Loss: 1.4914 | Val NLL: 1.1448\n",
      "Epoch 12 | Train Loss: 1.0921 | Train NLL: 0.7587 | Val Loss: 1.4687 | Val NLL: 1.1477\n",
      "Epoch 13 | Train Loss: 1.0558 | Train NLL: 0.7454 | Val Loss: 1.4515 | Val NLL: 1.1509\n",
      "Epoch 14 | Train Loss: 1.0241 | Train NLL: 0.7320 | Val Loss: 1.4381 | Val NLL: 1.1536\n",
      "Epoch 15 | Train Loss: 0.9959 | Train NLL: 0.7181 | Val Loss: 1.4302 | Val NLL: 1.1583\n",
      "Epoch 16 | Train Loss: 0.9708 | Train NLL: 0.7043 | Val Loss: 1.4243 | Val NLL: 1.1623\n",
      "Epoch 17 | Train Loss: 0.9482 | Train NLL: 0.6905 | Val Loss: 1.4201 | Val NLL: 1.1658\n",
      "Epoch 18 | Train Loss: 0.9272 | Train NLL: 0.6763 | Val Loss: 1.4198 | Val NLL: 1.1714\n",
      "Epoch 19 | Train Loss: 0.9078 | Train NLL: 0.6620 | Val Loss: 1.4229 | Val NLL: 1.1789\n",
      "Epoch 20 | Train Loss: 0.8900 | Train NLL: 0.6480 | Val Loss: 1.4252 | Val NLL: 1.1844\n"
     ]
    }
   ],
   "source": [
    "model = MLP(importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='overlapping'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188f952",
   "metadata": {},
   "source": [
    "## Dependent Censoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b9302fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_order = rs.permutation(len(X.T))\n",
    "T_features = feature_order[N_OVERLAPPING:(N_FEATURES + N_OVERLAPPING)]\n",
    "E_features = feature_order[N_FEATURES:(2 * N_FEATURES)]\n",
    "C_features = feature_order[(2 * N_FEATURES - N_OVERLAPPING):(3 * N_FEATURES - N_OVERLAPPING)]\n",
    "\n",
    "T_weights = rs.randn(len(T_features))\n",
    "E_weights = rs.randn(len(E_features))\n",
    "C_weights = rs.randn(len(C_features))\n",
    "\n",
    "T = X[:, T_features] @ T_weights\n",
    "E = X[:, E_features] @ E_weights\n",
    "C = X[:, C_features] @ C_weights\n",
    "\n",
    "t_cont = np.exp((T - np.mean(T)) / (2 * np.std(T)))\n",
    "c_cont = np.exp((C - np.mean(C)) / (2 * np.std(C)))\n",
    "\n",
    "e_prob = 1 - 1 / (1 + np.exp(4. * (E - E.mean()) / E.std()))\n",
    "e = (rs.rand(len(e_prob)) < e_prob).astype(int)\n",
    "\n",
    "ct_cont = np.stack([c_cont, t_cont]).T\n",
    "y_cont = e * np.amin(ct_cont, axis=1) + (1 - e) * c_cont\n",
    "\n",
    "s = e * np.argmin(ct_cont, axis=1)\n",
    "y_disc, t_disc, c_disc = discretize(y_cont, t_cont, c_cont, nbins=N_BINS)\n",
    "\n",
    "y = onehot(y_disc, ncategories=N_BINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38d3fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pstrue = s_train.mean()\n",
    "importance_weights = [1 / (1 - pstrue), 1 / pstrue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0893ea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many are susceptible?\n",
      "0    1234\n",
      "1    1168\n",
      "dtype: int64\n",
      "How many are observed?\n",
      "0    1838\n",
      "1     564\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('How many are susceptible?')\n",
    "print(pd.value_counts(e).sort_index())\n",
    "\n",
    "print('How many are observed?')\n",
    "print(pd.value_counts(s).sort_index())\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))\n",
    "\n",
    "ax[0].hist(e_prob)\n",
    "ax[1].hist(t_disc)\n",
    "ax[2].hist(c_disc)\n",
    "ax[3].hist(y_disc)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a735f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, s_train = X[:1500], y[:1500], s[:1500]\n",
    "x_val, y_val, s_val = X[1500:1900], y[1500:1900], s[1500:1900]\n",
    "x_test, y_test, s_test, e_test = X[1900:], y[1900:], s[1900:], e[1900:]\n",
    "t_disc_test, c_disc_test, y_disc_test = t_disc[1900:], c_disc[1900:], y_disc[1900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19d5226b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 12.8297 | Train NLL: 1.7107 | Val Loss: 10.2858 | Val NLL: 1.0183\n",
      "Epoch  1 | Train Loss: 8.5090 | Train NLL: 0.7315 | Val Loss: 7.1565 | Val NLL: 0.7874\n",
      "Epoch  2 | Train Loss: 5.8882 | Train NLL: 0.5393 | Val Loss: 5.0763 | Val NLL: 0.6720\n",
      "Epoch  3 | Train Loss: 4.1825 | Train NLL: 0.4494 | Val Loss: 3.7291 | Val NLL: 0.6139\n",
      "Epoch  4 | Train Loss: 3.0675 | Train NLL: 0.3929 | Val Loss: 2.8402 | Val NLL: 0.5716\n",
      "Epoch  5 | Train Loss: 2.3323 | Train NLL: 0.3556 | Val Loss: 2.2528 | Val NLL: 0.5456\n",
      "Epoch  6 | Train Loss: 1.8377 | Train NLL: 0.3279 | Val Loss: 1.8554 | Val NLL: 0.5287\n",
      "Epoch  7 | Train Loss: 1.4964 | Train NLL: 0.3065 | Val Loss: 1.5773 | Val NLL: 0.5146\n",
      "Epoch  8 | Train Loss: 1.2546 | Train NLL: 0.2893 | Val Loss: 1.3790 | Val NLL: 0.5046\n",
      "Epoch  9 | Train Loss: 1.0790 | Train NLL: 0.2758 | Val Loss: 1.2344 | Val NLL: 0.4974\n",
      "Epoch 10 | Train Loss: 0.9482 | Train NLL: 0.2643 | Val Loss: 1.1236 | Val NLL: 0.4889\n",
      "Epoch 11 | Train Loss: 0.8499 | Train NLL: 0.2556 | Val Loss: 1.0433 | Val NLL: 0.4862\n",
      "Epoch 12 | Train Loss: 0.7722 | Train NLL: 0.2465 | Val Loss: 0.9795 | Val NLL: 0.4823\n",
      "Epoch 13 | Train Loss: 0.7113 | Train NLL: 0.2389 | Val Loss: 0.9295 | Val NLL: 0.4795\n",
      "Epoch 14 | Train Loss: 0.6624 | Train NLL: 0.2323 | Val Loss: 0.8888 | Val NLL: 0.4763\n",
      "Epoch 15 | Train Loss: 0.6228 | Train NLL: 0.2265 | Val Loss: 0.8559 | Val NLL: 0.4738\n",
      "Epoch 16 | Train Loss: 0.5907 | Train NLL: 0.2218 | Val Loss: 0.8325 | Val NLL: 0.4749\n",
      "Epoch 17 | Train Loss: 0.5627 | Train NLL: 0.2162 | Val Loss: 0.8072 | Val NLL: 0.4701\n",
      "Epoch 18 | Train Loss: 0.5391 | Train NLL: 0.2114 | Val Loss: 0.7887 | Val NLL: 0.4688\n",
      "Epoch 19 | Train Loss: 0.5190 | Train NLL: 0.2072 | Val Loss: 0.7731 | Val NLL: 0.4678\n",
      "Epoch 20 | Train Loss: 0.5018 | Train NLL: 0.2035 | Val Loss: 0.7609 | Val NLL: 0.4679\n",
      "Epoch 21 | Train Loss: 0.4863 | Train NLL: 0.1994 | Val Loss: 0.7478 | Val NLL: 0.4656\n",
      "Epoch 22 | Train Loss: 0.4731 | Train NLL: 0.1962 | Val Loss: 0.7371 | Val NLL: 0.4644\n",
      "Epoch 23 | Train Loss: 0.4608 | Train NLL: 0.1927 | Val Loss: 0.7288 | Val NLL: 0.4642\n",
      "Epoch 24 | Train Loss: 0.4499 | Train NLL: 0.1895 | Val Loss: 0.7201 | Val NLL: 0.4627\n",
      "Epoch 25 | Train Loss: 0.4401 | Train NLL: 0.1866 | Val Loss: 0.7128 | Val NLL: 0.4619\n",
      "Epoch 26 | Train Loss: 0.4311 | Train NLL: 0.1837 | Val Loss: 0.7064 | Val NLL: 0.4613\n",
      "Epoch 27 | Train Loss: 0.4230 | Train NLL: 0.1811 | Val Loss: 0.7009 | Val NLL: 0.4610\n",
      "Epoch 28 | Train Loss: 0.4152 | Train NLL: 0.1782 | Val Loss: 0.6957 | Val NLL: 0.4605\n",
      "Epoch 29 | Train Loss: 0.4085 | Train NLL: 0.1760 | Val Loss: 0.6916 | Val NLL: 0.4607\n",
      "Epoch 30 | Train Loss: 0.4017 | Train NLL: 0.1732 | Val Loss: 0.6870 | Val NLL: 0.4599\n",
      "Epoch 31 | Train Loss: 0.3956 | Train NLL: 0.1709 | Val Loss: 0.6838 | Val NLL: 0.4604\n",
      "Epoch 32 | Train Loss: 0.3895 | Train NLL: 0.1682 | Val Loss: 0.6796 | Val NLL: 0.4595\n",
      "Epoch 33 | Train Loss: 0.3842 | Train NLL: 0.1662 | Val Loss: 0.6774 | Val NLL: 0.4604\n",
      "Epoch 34 | Train Loss: 0.3792 | Train NLL: 0.1640 | Val Loss: 0.6753 | Val NLL: 0.4609\n",
      "Epoch 35 | Train Loss: 0.3742 | Train NLL: 0.1617 | Val Loss: 0.6723 | Val NLL: 0.4606\n",
      "Epoch 36 | Train Loss: 0.3693 | Train NLL: 0.1593 | Val Loss: 0.6702 | Val NLL: 0.4610\n",
      "Epoch 37 | Train Loss: 0.3650 | Train NLL: 0.1573 | Val Loss: 0.6677 | Val NLL: 0.4608\n",
      "Epoch 38 | Train Loss: 0.3605 | Train NLL: 0.1550 | Val Loss: 0.6658 | Val NLL: 0.4609\n",
      "Epoch 39 | Train Loss: 0.3565 | Train NLL: 0.1530 | Val Loss: 0.6650 | Val NLL: 0.4620\n",
      "Epoch 40 | Train Loss: 0.3524 | Train NLL: 0.1508 | Val Loss: 0.6646 | Val NLL: 0.4635\n",
      "Epoch 41 | Train Loss: 0.3484 | Train NLL: 0.1486 | Val Loss: 0.6630 | Val NLL: 0.4636\n",
      "Epoch 42 | Train Loss: 0.3446 | Train NLL: 0.1466 | Val Loss: 0.6633 | Val NLL: 0.4655\n",
      "Epoch 43 | Train Loss: 0.3409 | Train NLL: 0.1445 | Val Loss: 0.6619 | Val NLL: 0.4659\n",
      "Epoch 44 | Train Loss: 0.3373 | Train NLL: 0.1426 | Val Loss: 0.6614 | Val NLL: 0.4669\n",
      "Epoch 45 | Train Loss: 0.3342 | Train NLL: 0.1409 | Val Loss: 0.6633 | Val NLL: 0.4702\n",
      "Epoch 46 | Train Loss: 0.3309 | Train NLL: 0.1391 | Val Loss: 0.6594 | Val NLL: 0.4677\n",
      "Epoch 47 | Train Loss: 0.3276 | Train NLL: 0.1372 | Val Loss: 0.6623 | Val NLL: 0.4721\n",
      "Epoch 48 | Train Loss: 0.3242 | Train NLL: 0.1353 | Val Loss: 0.6585 | Val NLL: 0.4696\n",
      "Epoch 49 | Train Loss: 0.3216 | Train NLL: 0.1339 | Val Loss: 0.6609 | Val NLL: 0.4732\n",
      "Epoch 50 | Train Loss: 0.3184 | Train NLL: 0.1320 | Val Loss: 0.6586 | Val NLL: 0.4722\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=1e-2, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='dependent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91cdb9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 12.8721 | Train NLL: 1.7193 | Val Loss: 10.3182 | Val NLL: 1.0227\n",
      "Epoch  1 | Train Loss: 8.5223 | Train NLL: 0.7302 | Val Loss: 7.1538 | Val NLL: 0.7797\n",
      "Epoch  2 | Train Loss: 5.8919 | Train NLL: 0.5461 | Val Loss: 5.0546 | Val NLL: 0.6596\n",
      "Epoch  3 | Train Loss: 4.1774 | Train NLL: 0.4581 | Val Loss: 3.7048 | Val NLL: 0.6076\n",
      "Epoch  4 | Train Loss: 3.0548 | Train NLL: 0.4022 | Val Loss: 2.8168 | Val NLL: 0.5728\n",
      "Epoch  5 | Train Loss: 2.3117 | Train NLL: 0.3629 | Val Loss: 2.2217 | Val NLL: 0.5447\n",
      "Epoch  6 | Train Loss: 1.8127 | Train NLL: 0.3346 | Val Loss: 1.8178 | Val NLL: 0.5238\n",
      "Epoch  7 | Train Loss: 1.4696 | Train NLL: 0.3126 | Val Loss: 1.5400 | Val NLL: 0.5102\n",
      "Epoch  8 | Train Loss: 1.2275 | Train NLL: 0.2947 | Val Loss: 1.3398 | Val NLL: 0.4973\n",
      "Epoch  9 | Train Loss: 1.0543 | Train NLL: 0.2817 | Val Loss: 1.1973 | Val NLL: 0.4897\n",
      "Epoch 10 | Train Loss: 0.9251 | Train NLL: 0.2692 | Val Loss: 1.0908 | Val NLL: 0.4829\n",
      "Epoch 11 | Train Loss: 0.8270 | Train NLL: 0.2583 | Val Loss: 1.0093 | Val NLL: 0.4767\n",
      "Epoch 12 | Train Loss: 0.7523 | Train NLL: 0.2498 | Val Loss: 0.9475 | Val NLL: 0.4725\n",
      "Epoch 13 | Train Loss: 0.6940 | Train NLL: 0.2425 | Val Loss: 0.8991 | Val NLL: 0.4689\n",
      "Epoch 14 | Train Loss: 0.6472 | Train NLL: 0.2355 | Val Loss: 0.8611 | Val NLL: 0.4663\n",
      "Epoch 15 | Train Loss: 0.6090 | Train NLL: 0.2292 | Val Loss: 0.8293 | Val NLL: 0.4628\n",
      "Epoch 16 | Train Loss: 0.5783 | Train NLL: 0.2241 | Val Loss: 0.8052 | Val NLL: 0.4617\n",
      "Epoch 17 | Train Loss: 0.5525 | Train NLL: 0.2192 | Val Loss: 0.7845 | Val NLL: 0.4597\n",
      "Epoch 18 | Train Loss: 0.5306 | Train NLL: 0.2144 | Val Loss: 0.7674 | Val NLL: 0.4584\n",
      "Epoch 19 | Train Loss: 0.5118 | Train NLL: 0.2101 | Val Loss: 0.7543 | Val NLL: 0.4586\n",
      "Epoch 20 | Train Loss: 0.4954 | Train NLL: 0.2059 | Val Loss: 0.7422 | Val NLL: 0.4576\n",
      "Epoch 21 | Train Loss: 0.4809 | Train NLL: 0.2019 | Val Loss: 0.7326 | Val NLL: 0.4579\n",
      "Epoch 22 | Train Loss: 0.4679 | Train NLL: 0.1980 | Val Loss: 0.7243 | Val NLL: 0.4579\n",
      "Epoch 23 | Train Loss: 0.4560 | Train NLL: 0.1940 | Val Loss: 0.7155 | Val NLL: 0.4565\n",
      "Epoch 24 | Train Loss: 0.4455 | Train NLL: 0.1903 | Val Loss: 0.7076 | Val NLL: 0.4552\n",
      "Epoch 25 | Train Loss: 0.4355 | Train NLL: 0.1867 | Val Loss: 0.7006 | Val NLL: 0.4542\n",
      "Epoch 26 | Train Loss: 0.4265 | Train NLL: 0.1833 | Val Loss: 0.6941 | Val NLL: 0.4531\n",
      "Epoch 27 | Train Loss: 0.4187 | Train NLL: 0.1806 | Val Loss: 0.6905 | Val NLL: 0.4543\n",
      "Epoch 28 | Train Loss: 0.4115 | Train NLL: 0.1779 | Val Loss: 0.6863 | Val NLL: 0.4542\n",
      "Epoch 29 | Train Loss: 0.4045 | Train NLL: 0.1749 | Val Loss: 0.6826 | Val NLL: 0.4545\n",
      "Epoch 30 | Train Loss: 0.3979 | Train NLL: 0.1720 | Val Loss: 0.6801 | Val NLL: 0.4555\n",
      "Epoch 31 | Train Loss: 0.3917 | Train NLL: 0.1692 | Val Loss: 0.6753 | Val NLL: 0.4540\n",
      "Epoch 32 | Train Loss: 0.3857 | Train NLL: 0.1665 | Val Loss: 0.6729 | Val NLL: 0.4548\n",
      "Epoch 33 | Train Loss: 0.3802 | Train NLL: 0.1639 | Val Loss: 0.6706 | Val NLL: 0.4553\n",
      "Epoch 34 | Train Loss: 0.3750 | Train NLL: 0.1613 | Val Loss: 0.6694 | Val NLL: 0.4565\n",
      "Epoch 35 | Train Loss: 0.3700 | Train NLL: 0.1589 | Val Loss: 0.6685 | Val NLL: 0.4581\n",
      "Epoch 36 | Train Loss: 0.3652 | Train NLL: 0.1565 | Val Loss: 0.6662 | Val NLL: 0.4582\n",
      "Epoch 37 | Train Loss: 0.3606 | Train NLL: 0.1541 | Val Loss: 0.6646 | Val NLL: 0.4587\n",
      "Epoch 38 | Train Loss: 0.3561 | Train NLL: 0.1516 | Val Loss: 0.6647 | Val NLL: 0.4607\n",
      "Epoch 39 | Train Loss: 0.3521 | Train NLL: 0.1496 | Val Loss: 0.6645 | Val NLL: 0.4625\n",
      "Epoch 40 | Train Loss: 0.3482 | Train NLL: 0.1475 | Val Loss: 0.6643 | Val NLL: 0.4639\n",
      "Epoch 41 | Train Loss: 0.3442 | Train NLL: 0.1451 | Val Loss: 0.6630 | Val NLL: 0.4644\n",
      "Epoch 42 | Train Loss: 0.3403 | Train NLL: 0.1429 | Val Loss: 0.6622 | Val NLL: 0.4652\n",
      "Epoch 43 | Train Loss: 0.3368 | Train NLL: 0.1411 | Val Loss: 0.6647 | Val NLL: 0.4693\n",
      "Epoch 44 | Train Loss: 0.3333 | Train NLL: 0.1390 | Val Loss: 0.6633 | Val NLL: 0.4693\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=3e-3, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='dependent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d16736b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 12.7553 | Train NLL: 1.6323 | Val Loss: 10.2361 | Val NLL: 1.0037\n",
      "Epoch  1 | Train Loss: 8.4305 | Train NLL: 0.7150 | Val Loss: 7.0618 | Val NLL: 0.7725\n",
      "Epoch  2 | Train Loss: 5.7943 | Train NLL: 0.5309 | Val Loss: 4.9642 | Val NLL: 0.6483\n",
      "Epoch  3 | Train Loss: 4.0876 | Train NLL: 0.4414 | Val Loss: 3.6204 | Val NLL: 0.5896\n",
      "Epoch  4 | Train Loss: 2.9809 | Train NLL: 0.3858 | Val Loss: 2.7465 | Val NLL: 0.5520\n",
      "Epoch  5 | Train Loss: 2.2564 | Train NLL: 0.3481 | Val Loss: 2.1716 | Val NLL: 0.5271\n",
      "Epoch  6 | Train Loss: 1.7729 | Train NLL: 0.3206 | Val Loss: 1.7845 | Val NLL: 0.5100\n",
      "Epoch  7 | Train Loss: 1.4412 | Train NLL: 0.2991 | Val Loss: 1.5145 | Val NLL: 0.4955\n",
      "Epoch  8 | Train Loss: 1.2083 | Train NLL: 0.2830 | Val Loss: 1.3245 | Val NLL: 0.4864\n",
      "Epoch  9 | Train Loss: 1.0395 | Train NLL: 0.2693 | Val Loss: 1.1847 | Val NLL: 0.4778\n",
      "Epoch 10 | Train Loss: 0.9147 | Train NLL: 0.2582 | Val Loss: 1.0828 | Val NLL: 0.4731\n",
      "Epoch 11 | Train Loss: 0.8203 | Train NLL: 0.2490 | Val Loss: 1.0051 | Val NLL: 0.4691\n",
      "Epoch 12 | Train Loss: 0.7478 | Train NLL: 0.2414 | Val Loss: 0.9454 | Val NLL: 0.4661\n",
      "Epoch 13 | Train Loss: 0.6901 | Train NLL: 0.2341 | Val Loss: 0.8970 | Val NLL: 0.4620\n",
      "Epoch 14 | Train Loss: 0.6455 | Train NLL: 0.2290 | Val Loss: 0.8634 | Val NLL: 0.4634\n",
      "Epoch 15 | Train Loss: 0.6081 | Train NLL: 0.2232 | Val Loss: 0.8320 | Val NLL: 0.4602\n",
      "Epoch 16 | Train Loss: 0.5765 | Train NLL: 0.2172 | Val Loss: 0.8059 | Val NLL: 0.4572\n",
      "Epoch 17 | Train Loss: 0.5504 | Train NLL: 0.2122 | Val Loss: 0.7847 | Val NLL: 0.4552\n",
      "Epoch 18 | Train Loss: 0.5290 | Train NLL: 0.2084 | Val Loss: 0.7694 | Val NLL: 0.4558\n",
      "Epoch 19 | Train Loss: 0.5099 | Train NLL: 0.2039 | Val Loss: 0.7534 | Val NLL: 0.4532\n",
      "Epoch 20 | Train Loss: 0.4932 | Train NLL: 0.1997 | Val Loss: 0.7398 | Val NLL: 0.4512\n",
      "Epoch 21 | Train Loss: 0.4785 | Train NLL: 0.1957 | Val Loss: 0.7283 | Val NLL: 0.4497\n",
      "Epoch 22 | Train Loss: 0.4658 | Train NLL: 0.1924 | Val Loss: 0.7187 | Val NLL: 0.4489\n",
      "Epoch 23 | Train Loss: 0.4544 | Train NLL: 0.1891 | Val Loss: 0.7108 | Val NLL: 0.4486\n",
      "Epoch 24 | Train Loss: 0.4443 | Train NLL: 0.1861 | Val Loss: 0.7051 | Val NLL: 0.4494\n",
      "Epoch 25 | Train Loss: 0.4348 | Train NLL: 0.1830 | Val Loss: 0.6976 | Val NLL: 0.4482\n",
      "Epoch 26 | Train Loss: 0.4261 | Train NLL: 0.1801 | Val Loss: 0.6918 | Val NLL: 0.4478\n",
      "Epoch 27 | Train Loss: 0.4183 | Train NLL: 0.1774 | Val Loss: 0.6881 | Val NLL: 0.4488\n",
      "Epoch 28 | Train Loss: 0.4109 | Train NLL: 0.1745 | Val Loss: 0.6832 | Val NLL: 0.4483\n",
      "Epoch 29 | Train Loss: 0.4040 | Train NLL: 0.1717 | Val Loss: 0.6799 | Val NLL: 0.4489\n",
      "Epoch 30 | Train Loss: 0.3977 | Train NLL: 0.1693 | Val Loss: 0.6788 | Val NLL: 0.4515\n",
      "Epoch 31 | Train Loss: 0.3916 | Train NLL: 0.1667 | Val Loss: 0.6752 | Val NLL: 0.4512\n",
      "Epoch 32 | Train Loss: 0.3858 | Train NLL: 0.1641 | Val Loss: 0.6725 | Val NLL: 0.4516\n",
      "Epoch 33 | Train Loss: 0.3805 | Train NLL: 0.1617 | Val Loss: 0.6711 | Val NLL: 0.4532\n",
      "Epoch 34 | Train Loss: 0.3756 | Train NLL: 0.1598 | Val Loss: 0.6689 | Val NLL: 0.4536\n",
      "Epoch 35 | Train Loss: 0.3706 | Train NLL: 0.1573 | Val Loss: 0.6695 | Val NLL: 0.4567\n",
      "Epoch 36 | Train Loss: 0.3660 | Train NLL: 0.1551 | Val Loss: 0.6670 | Val NLL: 0.4566\n",
      "Epoch 37 | Train Loss: 0.3615 | Train NLL: 0.1529 | Val Loss: 0.6676 | Val NLL: 0.4593\n",
      "Epoch 38 | Train Loss: 0.3572 | Train NLL: 0.1508 | Val Loss: 0.6654 | Val NLL: 0.4593\n",
      "Epoch 39 | Train Loss: 0.3530 | Train NLL: 0.1486 | Val Loss: 0.6637 | Val NLL: 0.4595\n",
      "Epoch 40 | Train Loss: 0.3494 | Train NLL: 0.1470 | Val Loss: 0.6656 | Val NLL: 0.4632\n",
      "Epoch 41 | Train Loss: 0.3454 | Train NLL: 0.1447 | Val Loss: 0.6633 | Val NLL: 0.4627\n",
      "Epoch 42 | Train Loss: 0.3415 | Train NLL: 0.1425 | Val Loss: 0.6638 | Val NLL: 0.4649\n",
      "Epoch 43 | Train Loss: 0.3379 | Train NLL: 0.1406 | Val Loss: 0.6617 | Val NLL: 0.4644\n",
      "Epoch 44 | Train Loss: 0.3344 | Train NLL: 0.1387 | Val Loss: 0.6630 | Val NLL: 0.4672\n",
      "Epoch 45 | Train Loss: 0.3306 | Train NLL: 0.1365 | Val Loss: 0.6609 | Val NLL: 0.4667\n",
      "Epoch 46 | Train Loss: 0.3272 | Train NLL: 0.1346 | Val Loss: 0.6602 | Val NLL: 0.4675\n",
      "Epoch 47 | Train Loss: 0.3239 | Train NLL: 0.1328 | Val Loss: 0.6598 | Val NLL: 0.4685\n",
      "Epoch 48 | Train Loss: 0.3208 | Train NLL: 0.1310 | Val Loss: 0.6570 | Val NLL: 0.4671\n",
      "Epoch 49 | Train Loss: 0.3173 | Train NLL: 0.1289 | Val Loss: 0.6564 | Val NLL: 0.4679\n",
      "Epoch 50 | Train Loss: 0.3142 | Train NLL: 0.1272 | Val Loss: 0.6553 | Val NLL: 0.4682\n",
      "Epoch 51 | Train Loss: 0.3115 | Train NLL: 0.1257 | Val Loss: 0.6562 | Val NLL: 0.4702\n",
      "Epoch 52 | Train Loss: 0.3088 | Train NLL: 0.1241 | Val Loss: 0.6548 | Val NLL: 0.4700\n",
      "Epoch 53 | Train Loss: 0.3059 | Train NLL: 0.1224 | Val Loss: 0.6582 | Val NLL: 0.4745\n",
      "Epoch 54 | Train Loss: 0.3035 | Train NLL: 0.1212 | Val Loss: 0.6545 | Val NLL: 0.4720\n",
      "Epoch 55 | Train Loss: 0.3002 | Train NLL: 0.1191 | Val Loss: 0.6542 | Val NLL: 0.4729\n",
      "Epoch 56 | Train Loss: 0.2977 | Train NLL: 0.1177 | Val Loss: 0.6542 | Val NLL: 0.4739\n",
      "Epoch 57 | Train Loss: 0.2956 | Train NLL: 0.1166 | Val Loss: 0.6555 | Val NLL: 0.4761\n",
      "Epoch 58 | Train Loss: 0.2931 | Train NLL: 0.1150 | Val Loss: 0.6545 | Val NLL: 0.4761\n"
     ]
    }
   ],
   "source": [
    "model = DNMC(n_bins=N_BINS, importance_weights=importance_weights, ld=3e-2, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='dependent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f410c730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 10.9948 | Train NLL: 1.9170 | Val Loss: 9.0338 | Val NLL: 1.2208\n",
      "Epoch  1 | Train Loss: 7.5460 | Train NLL: 0.8306 | Val Loss: 6.5570 | Val NLL: 0.8974\n",
      "Epoch  2 | Train Loss: 5.4740 | Train NLL: 0.6175 | Val Loss: 4.8172 | Val NLL: 0.7235\n",
      "Epoch  3 | Train Loss: 4.0279 | Train NLL: 0.5020 | Val Loss: 3.6205 | Val NLL: 0.6315\n",
      "Epoch  4 | Train Loss: 3.0272 | Train NLL: 0.4356 | Val Loss: 2.8033 | Val NLL: 0.5861\n",
      "Epoch  5 | Train Loss: 2.3307 | Train NLL: 0.3909 | Val Loss: 2.2399 | Val NLL: 0.5603\n",
      "Epoch  6 | Train Loss: 1.8431 | Train NLL: 0.3578 | Val Loss: 1.8451 | Val NLL: 0.5414\n",
      "Epoch  7 | Train Loss: 1.4990 | Train NLL: 0.3323 | Val Loss: 1.5669 | Val NLL: 0.5281\n",
      "Epoch  8 | Train Loss: 1.2523 | Train NLL: 0.3115 | Val Loss: 1.3644 | Val NLL: 0.5151\n",
      "Epoch  9 | Train Loss: 1.0734 | Train NLL: 0.2953 | Val Loss: 1.2190 | Val NLL: 0.5074\n",
      "Epoch 10 | Train Loss: 0.9402 | Train NLL: 0.2813 | Val Loss: 1.1107 | Val NLL: 0.5011\n",
      "Epoch 11 | Train Loss: 0.8395 | Train NLL: 0.2697 | Val Loss: 1.0280 | Val NLL: 0.4953\n",
      "Epoch 12 | Train Loss: 0.7625 | Train NLL: 0.2603 | Val Loss: 0.9661 | Val NLL: 0.4922\n",
      "Epoch 13 | Train Loss: 0.7012 | Train NLL: 0.2512 | Val Loss: 0.9146 | Val NLL: 0.4868\n",
      "Epoch 14 | Train Loss: 0.6530 | Train NLL: 0.2441 | Val Loss: 0.8761 | Val NLL: 0.4847\n",
      "Epoch 15 | Train Loss: 0.6128 | Train NLL: 0.2366 | Val Loss: 0.8392 | Val NLL: 0.4770\n",
      "Epoch 16 | Train Loss: 0.5801 | Train NLL: 0.2302 | Val Loss: 0.8137 | Val NLL: 0.4748\n",
      "Epoch 17 | Train Loss: 0.5514 | Train NLL: 0.2226 | Val Loss: 0.7853 | Val NLL: 0.4655\n",
      "Epoch 18 | Train Loss: 0.5277 | Train NLL: 0.2163 | Val Loss: 0.7658 | Val NLL: 0.4616\n",
      "Epoch 19 | Train Loss: 0.5071 | Train NLL: 0.2101 | Val Loss: 0.7482 | Val NLL: 0.4573\n",
      "Epoch 20 | Train Loss: 0.4894 | Train NLL: 0.2046 | Val Loss: 0.7317 | Val NLL: 0.4522\n",
      "Epoch 21 | Train Loss: 0.4740 | Train NLL: 0.1999 | Val Loss: 0.7199 | Val NLL: 0.4502\n",
      "Epoch 22 | Train Loss: 0.4605 | Train NLL: 0.1955 | Val Loss: 0.7102 | Val NLL: 0.4490\n",
      "Epoch 23 | Train Loss: 0.4484 | Train NLL: 0.1914 | Val Loss: 0.7007 | Val NLL: 0.4470\n",
      "Epoch 24 | Train Loss: 0.4375 | Train NLL: 0.1876 | Val Loss: 0.6925 | Val NLL: 0.4454\n",
      "Epoch 25 | Train Loss: 0.4275 | Train NLL: 0.1838 | Val Loss: 0.6859 | Val NLL: 0.4446\n",
      "Epoch 26 | Train Loss: 0.4185 | Train NLL: 0.1804 | Val Loss: 0.6800 | Val NLL: 0.4440\n",
      "Epoch 27 | Train Loss: 0.4100 | Train NLL: 0.1769 | Val Loss: 0.6751 | Val NLL: 0.4438\n",
      "Epoch 28 | Train Loss: 0.4022 | Train NLL: 0.1736 | Val Loss: 0.6704 | Val NLL: 0.4434\n",
      "Epoch 29 | Train Loss: 0.3950 | Train NLL: 0.1704 | Val Loss: 0.6671 | Val NLL: 0.4438\n",
      "Epoch 30 | Train Loss: 0.3881 | Train NLL: 0.1672 | Val Loss: 0.6640 | Val NLL: 0.4443\n",
      "Epoch 31 | Train Loss: 0.3816 | Train NLL: 0.1640 | Val Loss: 0.6596 | Val NLL: 0.4431\n",
      "Epoch 32 | Train Loss: 0.3757 | Train NLL: 0.1613 | Val Loss: 0.6582 | Val NLL: 0.4448\n",
      "Epoch 33 | Train Loss: 0.3698 | Train NLL: 0.1584 | Val Loss: 0.6532 | Val NLL: 0.4427\n",
      "Epoch 34 | Train Loss: 0.3645 | Train NLL: 0.1557 | Val Loss: 0.6538 | Val NLL: 0.4456\n",
      "Epoch 35 | Train Loss: 0.3591 | Train NLL: 0.1529 | Val Loss: 0.6501 | Val NLL: 0.4446\n",
      "Epoch 36 | Train Loss: 0.3544 | Train NLL: 0.1507 | Val Loss: 0.6493 | Val NLL: 0.4460\n",
      "Epoch 37 | Train Loss: 0.3495 | Train NLL: 0.1479 | Val Loss: 0.6470 | Val NLL: 0.4460\n",
      "Epoch 38 | Train Loss: 0.3448 | Train NLL: 0.1455 | Val Loss: 0.6463 | Val NLL: 0.4475\n",
      "Epoch 39 | Train Loss: 0.3407 | Train NLL: 0.1435 | Val Loss: 0.6454 | Val NLL: 0.4485\n",
      "Epoch 40 | Train Loss: 0.3362 | Train NLL: 0.1409 | Val Loss: 0.6434 | Val NLL: 0.4484\n",
      "Epoch 41 | Train Loss: 0.3325 | Train NLL: 0.1390 | Val Loss: 0.6424 | Val NLL: 0.4492\n",
      "Epoch 42 | Train Loss: 0.3283 | Train NLL: 0.1367 | Val Loss: 0.6419 | Val NLL: 0.4505\n",
      "Epoch 43 | Train Loss: 0.3247 | Train NLL: 0.1348 | Val Loss: 0.6398 | Val NLL: 0.4502\n",
      "Epoch 44 | Train Loss: 0.3211 | Train NLL: 0.1328 | Val Loss: 0.6409 | Val NLL: 0.4529\n",
      "Epoch 45 | Train Loss: 0.3177 | Train NLL: 0.1313 | Val Loss: 0.6376 | Val NLL: 0.4519\n",
      "Epoch 46 | Train Loss: 0.3145 | Train NLL: 0.1299 | Val Loss: 0.6398 | Val NLL: 0.4553\n",
      "Epoch 47 | Train Loss: 0.3109 | Train NLL: 0.1276 | Val Loss: 0.6369 | Val NLL: 0.4535\n",
      "Epoch 48 | Train Loss: 0.3079 | Train NLL: 0.1259 | Val Loss: 0.6377 | Val NLL: 0.4558\n",
      "Epoch 49 | Train Loss: 0.3050 | Train NLL: 0.1245 | Val Loss: 0.6366 | Val NLL: 0.4566\n",
      "Epoch 50 | Train Loss: 0.3018 | Train NLL: 0.1228 | Val Loss: 0.6367 | Val NLL: 0.4576\n",
      "Epoch 51 | Train Loss: 0.2989 | Train NLL: 0.1210 | Val Loss: 0.6355 | Val NLL: 0.4575\n",
      "Epoch 52 | Train Loss: 0.2960 | Train NLL: 0.1193 | Val Loss: 0.6340 | Val NLL: 0.4577\n",
      "Epoch 53 | Train Loss: 0.2935 | Train NLL: 0.1183 | Val Loss: 0.6356 | Val NLL: 0.4603\n",
      "Epoch 54 | Train Loss: 0.2907 | Train NLL: 0.1165 | Val Loss: 0.6357 | Val NLL: 0.4612\n"
     ]
    }
   ],
   "source": [
    "model = NMC(n_bins=N_BINS, importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='dependent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20eaa4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 7.9641 | Train NLL: 1.7512 | Val Loss: 6.6249 | Val NLL: 1.1184\n",
      "Epoch  1 | Train Loss: 5.6065 | Train NLL: 0.7541 | Val Loss: 5.0321 | Val NLL: 0.8261\n",
      "Epoch  2 | Train Loss: 4.2560 | Train NLL: 0.5620 | Val Loss: 3.8715 | Val NLL: 0.6721\n",
      "Epoch  3 | Train Loss: 3.2803 | Train NLL: 0.4616 | Val Loss: 3.0497 | Val NLL: 0.5981\n",
      "Epoch  4 | Train Loss: 2.5738 | Train NLL: 0.4030 | Val Loss: 2.4603 | Val NLL: 0.5596\n",
      "Epoch  5 | Train Loss: 2.0572 | Train NLL: 0.3633 | Val Loss: 2.0305 | Val NLL: 0.5349\n",
      "Epoch  6 | Train Loss: 1.6771 | Train NLL: 0.3337 | Val Loss: 1.7138 | Val NLL: 0.5162\n",
      "Epoch  7 | Train Loss: 1.3945 | Train NLL: 0.3096 | Val Loss: 1.4779 | Val NLL: 0.5011\n",
      "Epoch  8 | Train Loss: 1.1836 | Train NLL: 0.2909 | Val Loss: 1.3007 | Val NLL: 0.4887\n",
      "Epoch  9 | Train Loss: 1.0243 | Train NLL: 0.2758 | Val Loss: 1.1672 | Val NLL: 0.4796\n",
      "Epoch 10 | Train Loss: 0.9023 | Train NLL: 0.2629 | Val Loss: 1.0644 | Val NLL: 0.4714\n",
      "Epoch 11 | Train Loss: 0.8085 | Train NLL: 0.2526 | Val Loss: 0.9870 | Val NLL: 0.4669\n",
      "Epoch 12 | Train Loss: 0.7334 | Train NLL: 0.2423 | Val Loss: 0.9231 | Val NLL: 0.4599\n",
      "Epoch 13 | Train Loss: 0.6744 | Train NLL: 0.2341 | Val Loss: 0.8765 | Val NLL: 0.4581\n",
      "Epoch 14 | Train Loss: 0.6263 | Train NLL: 0.2261 | Val Loss: 0.8363 | Val NLL: 0.4534\n",
      "Epoch 15 | Train Loss: 0.5869 | Train NLL: 0.2187 | Val Loss: 0.8049 | Val NLL: 0.4507\n",
      "Epoch 16 | Train Loss: 0.5549 | Train NLL: 0.2127 | Val Loss: 0.7822 | Val NLL: 0.4511\n",
      "Epoch 17 | Train Loss: 0.5274 | Train NLL: 0.2063 | Val Loss: 0.7601 | Val NLL: 0.4482\n",
      "Epoch 18 | Train Loss: 0.5044 | Train NLL: 0.2009 | Val Loss: 0.7432 | Val NLL: 0.4473\n",
      "Epoch 19 | Train Loss: 0.4844 | Train NLL: 0.1957 | Val Loss: 0.7272 | Val NLL: 0.4449\n",
      "Epoch 20 | Train Loss: 0.4673 | Train NLL: 0.1911 | Val Loss: 0.7149 | Val NLL: 0.4442\n",
      "Epoch 21 | Train Loss: 0.4521 | Train NLL: 0.1866 | Val Loss: 0.7031 | Val NLL: 0.4424\n",
      "Epoch 22 | Train Loss: 0.4388 | Train NLL: 0.1827 | Val Loss: 0.6934 | Val NLL: 0.4414\n",
      "Epoch 23 | Train Loss: 0.4269 | Train NLL: 0.1789 | Val Loss: 0.6855 | Val NLL: 0.4409\n",
      "Epoch 24 | Train Loss: 0.4161 | Train NLL: 0.1752 | Val Loss: 0.6772 | Val NLL: 0.4393\n",
      "Epoch 25 | Train Loss: 0.4063 | Train NLL: 0.1717 | Val Loss: 0.6711 | Val NLL: 0.4391\n",
      "Epoch 26 | Train Loss: 0.3975 | Train NLL: 0.1685 | Val Loss: 0.6648 | Val NLL: 0.4381\n",
      "Epoch 27 | Train Loss: 0.3893 | Train NLL: 0.1654 | Val Loss: 0.6593 | Val NLL: 0.4374\n",
      "Epoch 28 | Train Loss: 0.3817 | Train NLL: 0.1623 | Val Loss: 0.6552 | Val NLL: 0.4377\n",
      "Epoch 29 | Train Loss: 0.3747 | Train NLL: 0.1595 | Val Loss: 0.6512 | Val NLL: 0.4375\n",
      "Epoch 30 | Train Loss: 0.3680 | Train NLL: 0.1565 | Val Loss: 0.6462 | Val NLL: 0.4362\n",
      "Epoch 31 | Train Loss: 0.3618 | Train NLL: 0.1539 | Val Loss: 0.6429 | Val NLL: 0.4363\n",
      "Epoch 32 | Train Loss: 0.3560 | Train NLL: 0.1513 | Val Loss: 0.6405 | Val NLL: 0.4371\n",
      "Epoch 33 | Train Loss: 0.3506 | Train NLL: 0.1489 | Val Loss: 0.6391 | Val NLL: 0.4384\n",
      "Epoch 34 | Train Loss: 0.3453 | Train NLL: 0.1463 | Val Loss: 0.6362 | Val NLL: 0.4383\n",
      "Epoch 35 | Train Loss: 0.3403 | Train NLL: 0.1441 | Val Loss: 0.6343 | Val NLL: 0.4388\n",
      "Epoch 36 | Train Loss: 0.3357 | Train NLL: 0.1419 | Val Loss: 0.6332 | Val NLL: 0.4401\n",
      "Epoch 37 | Train Loss: 0.3313 | Train NLL: 0.1398 | Val Loss: 0.6314 | Val NLL: 0.4405\n",
      "Epoch 38 | Train Loss: 0.3269 | Train NLL: 0.1374 | Val Loss: 0.6294 | Val NLL: 0.4406\n",
      "Epoch 39 | Train Loss: 0.3226 | Train NLL: 0.1352 | Val Loss: 0.6283 | Val NLL: 0.4415\n",
      "Epoch 40 | Train Loss: 0.3186 | Train NLL: 0.1331 | Val Loss: 0.6270 | Val NLL: 0.4421\n",
      "Epoch 41 | Train Loss: 0.3147 | Train NLL: 0.1312 | Val Loss: 0.6250 | Val NLL: 0.4420\n",
      "Epoch 42 | Train Loss: 0.3110 | Train NLL: 0.1292 | Val Loss: 0.6256 | Val NLL: 0.4442\n",
      "Epoch 43 | Train Loss: 0.3074 | Train NLL: 0.1272 | Val Loss: 0.6229 | Val NLL: 0.4432\n",
      "Epoch 44 | Train Loss: 0.3040 | Train NLL: 0.1256 | Val Loss: 0.6238 | Val NLL: 0.4458\n",
      "Epoch 45 | Train Loss: 0.3006 | Train NLL: 0.1238 | Val Loss: 0.6222 | Val NLL: 0.4457\n",
      "Epoch 46 | Train Loss: 0.2977 | Train NLL: 0.1223 | Val Loss: 0.6222 | Val NLL: 0.4472\n",
      "Epoch 47 | Train Loss: 0.2943 | Train NLL: 0.1205 | Val Loss: 0.6209 | Val NLL: 0.4473\n",
      "Epoch 48 | Train Loss: 0.2912 | Train NLL: 0.1188 | Val Loss: 0.6211 | Val NLL: 0.4488\n",
      "Epoch 49 | Train Loss: 0.2882 | Train NLL: 0.1170 | Val Loss: 0.6183 | Val NLL: 0.4474\n",
      "Epoch 50 | Train Loss: 0.2854 | Train NLL: 0.1156 | Val Loss: 0.6206 | Val NLL: 0.4509\n",
      "Epoch 51 | Train Loss: 0.2827 | Train NLL: 0.1141 | Val Loss: 0.6206 | Val NLL: 0.4521\n"
     ]
    }
   ],
   "source": [
    "model = NSurv(n_bins=N_BINS, importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='dependent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a949809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Train Loss: 4.0572 | Train NLL: 1.0836 | Val Loss: 3.5114 | Val NLL: 0.9406\n",
      "Epoch  1 | Train Loss: 3.1001 | Train NLL: 0.8460 | Val Loss: 2.7532 | Val NLL: 0.8030\n",
      "Epoch  2 | Train Loss: 2.4219 | Train NLL: 0.7027 | Val Loss: 2.2211 | Val NLL: 0.7224\n",
      "Epoch  3 | Train Loss: 1.9446 | Train NLL: 0.6126 | Val Loss: 1.8563 | Val NLL: 0.6834\n",
      "Epoch  4 | Train Loss: 1.6068 | Train NLL: 0.5540 | Val Loss: 1.5909 | Val NLL: 0.6527\n",
      "Epoch  5 | Train Loss: 1.3620 | Train NLL: 0.5107 | Val Loss: 1.4024 | Val NLL: 0.6340\n",
      "Epoch  6 | Train Loss: 1.1808 | Train NLL: 0.4760 | Val Loss: 1.2654 | Val NLL: 0.6211\n",
      "Epoch  7 | Train Loss: 1.0444 | Train NLL: 0.4471 | Val Loss: 1.1646 | Val NLL: 0.6123\n",
      "Epoch  8 | Train Loss: 0.9392 | Train NLL: 0.4221 | Val Loss: 1.0890 | Val NLL: 0.6055\n",
      "Epoch  9 | Train Loss: 0.8573 | Train NLL: 0.4007 | Val Loss: 1.0334 | Val NLL: 0.6024\n",
      "Epoch 10 | Train Loss: 0.7922 | Train NLL: 0.3819 | Val Loss: 0.9907 | Val NLL: 0.6001\n",
      "Epoch 11 | Train Loss: 0.7394 | Train NLL: 0.3649 | Val Loss: 0.9564 | Val NLL: 0.5974\n",
      "Epoch 12 | Train Loss: 0.6963 | Train NLL: 0.3501 | Val Loss: 0.9309 | Val NLL: 0.5968\n",
      "Epoch 13 | Train Loss: 0.6609 | Train NLL: 0.3371 | Val Loss: 0.9116 | Val NLL: 0.5975\n",
      "Epoch 14 | Train Loss: 0.6304 | Train NLL: 0.3247 | Val Loss: 0.8956 | Val NLL: 0.5978\n",
      "Epoch 15 | Train Loss: 0.6040 | Train NLL: 0.3131 | Val Loss: 0.8811 | Val NLL: 0.5966\n",
      "Epoch 16 | Train Loss: 0.5816 | Train NLL: 0.3027 | Val Loss: 0.8721 | Val NLL: 0.5984\n",
      "Epoch 17 | Train Loss: 0.5620 | Train NLL: 0.2931 | Val Loss: 0.8647 | Val NLL: 0.6001\n",
      "Epoch 18 | Train Loss: 0.5443 | Train NLL: 0.2836 | Val Loss: 0.8573 | Val NLL: 0.6002\n",
      "Epoch 19 | Train Loss: 0.5289 | Train NLL: 0.2751 | Val Loss: 0.8522 | Val NLL: 0.6016\n",
      "Epoch 20 | Train Loss: 0.5147 | Train NLL: 0.2669 | Val Loss: 0.8492 | Val NLL: 0.6041\n",
      "Epoch 21 | Train Loss: 0.5022 | Train NLL: 0.2596 | Val Loss: 0.8445 | Val NLL: 0.6042\n",
      "Epoch 22 | Train Loss: 0.4904 | Train NLL: 0.2523 | Val Loss: 0.8432 | Val NLL: 0.6071\n",
      "Epoch 23 | Train Loss: 0.4797 | Train NLL: 0.2455 | Val Loss: 0.8436 | Val NLL: 0.6111\n",
      "Epoch 24 | Train Loss: 0.4699 | Train NLL: 0.2393 | Val Loss: 0.8423 | Val NLL: 0.6132\n",
      "Epoch 25 | Train Loss: 0.4604 | Train NLL: 0.2329 | Val Loss: 0.8408 | Val NLL: 0.6147\n",
      "Epoch 26 | Train Loss: 0.4518 | Train NLL: 0.2272 | Val Loss: 0.8420 | Val NLL: 0.6186\n",
      "Epoch 27 | Train Loss: 0.4437 | Train NLL: 0.2217 | Val Loss: 0.8409 | Val NLL: 0.6201\n"
     ]
    }
   ],
   "source": [
    "model = MLP(importance_weights=importance_weights, lr=1e-2)\n",
    "train_model(model, (x_train, y_train, s_train), (x_val, y_val, s_val), 100, learning_rate=LEARNING_RATE)\n",
    "all_results.append(evaluate_model(model, (x_test, y_test, s_test), e_test, factors='dependent'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c645a5ce",
   "metadata": {},
   "source": [
    "# Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e3072e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>factors</th>\n",
       "      <th>model</th>\n",
       "      <th>ld</th>\n",
       "      <th>lr</th>\n",
       "      <th>avg_test_loss</th>\n",
       "      <th>avg_test_nll</th>\n",
       "      <th>e_auc</th>\n",
       "      <th>y_ci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nacd</td>\n",
       "      <td>separate</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.904205</td>\n",
       "      <td>0.888634</td>\n",
       "      <td>0.665129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nacd</td>\n",
       "      <td>separate</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.940007</td>\n",
       "      <td>0.887542</td>\n",
       "      <td>0.668540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nacd</td>\n",
       "      <td>separate</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.930373</td>\n",
       "      <td>0.889276</td>\n",
       "      <td>0.669591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nacd</td>\n",
       "      <td>separate</td>\n",
       "      <td>NMC</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.594212</td>\n",
       "      <td>3.790836</td>\n",
       "      <td>0.896005</td>\n",
       "      <td>0.679137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nacd</td>\n",
       "      <td>separate</td>\n",
       "      <td>NSurv</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.646095</td>\n",
       "      <td>3.814530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.870522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nacd</td>\n",
       "      <td>separate</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.296782</td>\n",
       "      <td>1.012142</td>\n",
       "      <td>0.863839</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nacd</td>\n",
       "      <td>overlapping</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.412213</td>\n",
       "      <td>3.587344</td>\n",
       "      <td>0.916140</td>\n",
       "      <td>0.653026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nacd</td>\n",
       "      <td>overlapping</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.387992</td>\n",
       "      <td>3.567028</td>\n",
       "      <td>0.920801</td>\n",
       "      <td>0.674846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nacd</td>\n",
       "      <td>overlapping</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.370396</td>\n",
       "      <td>3.546387</td>\n",
       "      <td>0.920913</td>\n",
       "      <td>0.665059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nacd</td>\n",
       "      <td>overlapping</td>\n",
       "      <td>NMC</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.207434</td>\n",
       "      <td>3.466679</td>\n",
       "      <td>0.920993</td>\n",
       "      <td>0.640195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nacd</td>\n",
       "      <td>overlapping</td>\n",
       "      <td>NSurv</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.305937</td>\n",
       "      <td>3.543122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.859784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nacd</td>\n",
       "      <td>overlapping</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.207884</td>\n",
       "      <td>0.967087</td>\n",
       "      <td>0.909978</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nacd</td>\n",
       "      <td>dependent</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.281564</td>\n",
       "      <td>0.779079</td>\n",
       "      <td>0.817143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nacd</td>\n",
       "      <td>dependent</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.278199</td>\n",
       "      <td>0.762451</td>\n",
       "      <td>0.816320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nacd</td>\n",
       "      <td>dependent</td>\n",
       "      <td>DNMC</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.277759</td>\n",
       "      <td>0.740376</td>\n",
       "      <td>0.815394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nacd</td>\n",
       "      <td>dependent</td>\n",
       "      <td>NMC</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.451678</td>\n",
       "      <td>0.277214</td>\n",
       "      <td>0.601954</td>\n",
       "      <td>0.830829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nacd</td>\n",
       "      <td>dependent</td>\n",
       "      <td>NSurv</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.446458</td>\n",
       "      <td>0.277987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.830624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>nacd</td>\n",
       "      <td>dependent</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.784725</td>\n",
       "      <td>0.563908</td>\n",
       "      <td>0.648746</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset      factors  model     ld    lr  avg_test_loss  avg_test_nll  \\\n",
       "0     nacd     separate   DNMC  0.010  0.01            NaN      3.904205   \n",
       "1     nacd     separate   DNMC  0.003  0.01            NaN      3.940007   \n",
       "2     nacd     separate   DNMC  0.030  0.01            NaN      3.930373   \n",
       "3     nacd     separate    NMC  0.001  0.01       4.594212      3.790836   \n",
       "4     nacd     separate  NSurv  0.001  0.01       4.646095      3.814530   \n",
       "5     nacd     separate    MLP  0.001  0.01       1.296782      1.012142   \n",
       "6     nacd  overlapping   DNMC  0.010  0.01       4.412213      3.587344   \n",
       "7     nacd  overlapping   DNMC  0.003  0.01       4.387992      3.567028   \n",
       "8     nacd  overlapping   DNMC  0.030  0.01       4.370396      3.546387   \n",
       "9     nacd  overlapping    NMC  0.001  0.01       4.207434      3.466679   \n",
       "10    nacd  overlapping  NSurv  0.001  0.01       4.305937      3.543122   \n",
       "11    nacd  overlapping    MLP  0.001  0.01       1.207884      0.967087   \n",
       "12    nacd    dependent   DNMC  0.010  0.01            NaN      0.281564   \n",
       "13    nacd    dependent   DNMC  0.003  0.01            NaN      0.278199   \n",
       "14    nacd    dependent   DNMC  0.030  0.01            NaN      0.277759   \n",
       "15    nacd    dependent    NMC  0.001  0.01       0.451678      0.277214   \n",
       "16    nacd    dependent  NSurv  0.001  0.01       0.446458      0.277987   \n",
       "17    nacd    dependent    MLP  0.001  0.01       0.784725      0.563908   \n",
       "\n",
       "       e_auc      y_ci  \n",
       "0   0.888634  0.665129  \n",
       "1   0.887542  0.668540  \n",
       "2   0.889276  0.669591  \n",
       "3   0.896005  0.679137  \n",
       "4        NaN  0.870522  \n",
       "5   0.863839       NaN  \n",
       "6   0.916140  0.653026  \n",
       "7   0.920801  0.674846  \n",
       "8   0.920913  0.665059  \n",
       "9   0.920993  0.640195  \n",
       "10       NaN  0.859784  \n",
       "11  0.909978       NaN  \n",
       "12  0.779079  0.817143  \n",
       "13  0.762451  0.816320  \n",
       "14  0.740376  0.815394  \n",
       "15  0.601954  0.830829  \n",
       "16       NaN  0.830624  \n",
       "17  0.648746       NaN  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.DataFrame(all_results)\n",
    "summary.to_csv('~/Downloads/nacd_results.csv', index=False)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e04763",
   "metadata": {},
   "source": [
    "### Identification of Factors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7aeb943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trying to work toward evidence of identification of factors\n",
    "# # Phi -> E, Omega -> T\n",
    "\n",
    "# E_mask = np.array([False] * X.shape[1])\n",
    "# E_mask[E_features] = True\n",
    "\n",
    "# T_mask = np.array([False] * X.shape[1])\n",
    "# T_mask[T_features] = True\n",
    "\n",
    "# phi_on_e = np.abs(model.phi_layers[0].kernel.numpy()).max(axis=1)[E_mask].mean()\n",
    "# phi_off_e = np.abs(model.phi_layers[0].kernel.numpy()).max(axis=1)[~E_mask].mean()\n",
    "\n",
    "# omega_on_t = np.abs(model.omega_layers[0].kernel.numpy()).max(axis=1)[T_mask].mean()\n",
    "# omega_off_t = np.abs(model.omega_layers[0].kernel.numpy()).max(axis=1)[~T_mask].mean()\n",
    "\n",
    "# print(phi_on_e, phi_off_e, omega_on_t, omega_off_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
